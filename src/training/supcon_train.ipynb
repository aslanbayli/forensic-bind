{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_GL-W_5PhyB",
        "outputId": "adb68a40-0469-4167-b848-66363ca5673f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive\")\n",
        "\n",
        "DATA_ROOT = DRIVE_ROOT / \"forensicbind\"\n",
        "\n",
        "MANIPULATED_DIR = DATA_ROOT / \"manipulated_sequences\"\n",
        "ORIGINAL_DIR    = DATA_ROOT / \"original_sequences\"\n",
        "\n",
        "print(\"Manipulated exists:\", MANIPULATED_DIR.exists())\n",
        "print(\"Original exists   :\", ORIGINAL_DIR.exists())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UsRtIFzP0aR",
        "outputId": "14344ac6-41be-4343-a934-dd8931b8ecc3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manipulated exists: True\n",
            "Original exists   : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust these names if your folders are lowercase, e.g. \"deepfake\" instead of \"Deepfakes\"\n",
        "ORIGINAL_DIR = DATA_ROOT / \"original_sequences\" / \"c23\"\n",
        "MANIP_ROOT = DATA_ROOT / \"manipulated_sequences\"\n",
        "\n",
        "# 4-class setup: Real, Face2Face, FaceSwap, NeuralTextures\n",
        "# DeepFakes is *excluded* here on purpose\n",
        "METHOD_DIRS = {\n",
        "    \"Deepfakes\": MANIP_ROOT / \"Deepfakes\" / \"c23\",\n",
        "    \"Face2Face\": MANIP_ROOT / \"Face2Face\" / \"c23\",\n",
        "    \"FaceSwap\": MANIP_ROOT / \"FaceSwap\" / \"c23\",\n",
        "    \"NeuralTextures\": MANIP_ROOT / \"NeuralTextures\" / \"c23\",\n",
        "}\n",
        "print(\"Original dir exists:\", ORIGINAL_DIR.exists())\n",
        "for method, path in METHOD_DIRS.items():\n",
        "    print(f\"{method} dir:\", path, \"exists:\", path.exists())\n",
        "\n",
        "# Quick sanity: list a few video folders and frame counts\n",
        "def inspect_dir(dir_path, label):\n",
        "    video_dirs = sorted([p for p in dir_path.iterdir() if p.is_dir()])\n",
        "    print(f\"\\n[{label}] Found {len(video_dirs)} video folders\")\n",
        "    for vd in video_dirs[:3]:  # show first 3 only\n",
        "        frames = list(vd.glob(\"*.png\"))\n",
        "        print(\n",
        "            \"  \",\n",
        "            vd.name,\n",
        "            \"->\",\n",
        "            len(frames),\n",
        "            \"frames (example:\",\n",
        "            frames[0].name if frames else \"NO FRAMES\",\n",
        "            \")\",\n",
        "        )\n",
        "\n",
        "inspect_dir(ORIGINAL_DIR, \"Real\")\n",
        "for method, path in METHOD_DIRS.items():\n",
        "    inspect_dir(path, method)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXmiVSPyP73b",
        "outputId": "95b00c24-0ba0-4201-c710-2ea65f7999ba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dir exists: True\n",
            "Deepfakes dir: /content/drive/MyDrive/forensicbind/manipulated_sequences/Deepfakes/c23 exists: True\n",
            "Face2Face dir: /content/drive/MyDrive/forensicbind/manipulated_sequences/Face2Face/c23 exists: True\n",
            "FaceSwap dir: /content/drive/MyDrive/forensicbind/manipulated_sequences/FaceSwap/c23 exists: True\n",
            "NeuralTextures dir: /content/drive/MyDrive/forensicbind/manipulated_sequences/NeuralTextures/c23 exists: True\n",
            "\n",
            "[Real] Found 200 video folders\n",
            "   008.mp4 -> 387 frames (example: 00021_0.png )\n",
            "   009.mp4 -> 338 frames (example: 00042_0.png )\n",
            "   018.mp4 -> 182 frames (example: 00033_0.png )\n",
            "\n",
            "[Deepfakes] Found 101 video folders\n",
            "   000_003.mp4 -> 396 frames (example: 00000_0.png )\n",
            "   001_870.mp4 -> 460 frames (example: 00000_0.png )\n",
            "   002_006.mp4 -> 693 frames (example: 00000_0.png )\n",
            "\n",
            "[Face2Face] Found 130 video folders\n",
            "   000_003.mp4 -> 303 frames (example: 00000_0.png )\n",
            "   001_870.mp4 -> 604 frames (example: 00000_0.png )\n",
            "   002_006.mp4 -> 310 frames (example: 00000_0.png )\n",
            "\n",
            "[FaceSwap] Found 133 video folders\n",
            "   002_006.mp4 -> 310 frames (example: 00000_0.png )\n",
            "   003_000.mp4 -> 303 frames (example: 00000_0.png )\n",
            "   008_990.mp4 -> 305 frames (example: 00000_0.png )\n",
            "\n",
            "[NeuralTextures] Found 71 video folders\n",
            "   000_003.mp4 -> 303 frames (example: 00000_0.png )\n",
            "   001_870.mp4 -> 460 frames (example: 00000_0.png )\n",
            "   002_006.mp4 -> 310 frames (example: 00000_0.png )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def count_total_frames(dir_path):\n",
        "    total = 0\n",
        "    video_dirs = [p for p in dir_path.iterdir() if p.is_dir()]\n",
        "    for vd in video_dirs:\n",
        "        total += len(list(vd.glob(\"*.png\")))\n",
        "    return total, len(video_dirs)\n",
        "\n",
        "\n",
        "print(\"\\n===== FRAME COUNTS =====\")\n",
        "\n",
        "# Real\n",
        "real_frames, real_videos = count_total_frames(ORIGINAL_DIR)\n",
        "print(f\"Real           : {real_frames:,} frames from {real_videos} videos\")\n",
        "\n",
        "# Manipulated\n",
        "overall_frames = real_frames\n",
        "overall_videos = real_videos\n",
        "\n",
        "for method, path in METHOD_DIRS.items():\n",
        "    frames, videos = count_total_frames(path)\n",
        "    print(f\"{method:<15}: {frames:,} frames from {videos} videos\")\n",
        "    overall_frames += frames\n",
        "    overall_videos += videos\n",
        "\n",
        "print(\"\\n------------------------\")\n",
        "print(f\"TOTAL           : {overall_frames:,} frames from {overall_videos} videos\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_MzZTKYRiAd",
        "outputId": "6e68b81f-bbd5-417e-ece3-a1831ee11116"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== FRAME COUNTS =====\n",
            "Real           : 30,789 frames from 200 videos\n",
            "Deepfakes      : 48,091 frames from 101 videos\n",
            "Face2Face      : 64,105 frames from 130 videos\n",
            "FaceSwap       : 50,778 frames from 133 videos\n",
            "NeuralTextures : 26,954 frames from 71 videos\n",
            "\n",
            "------------------------\n",
            "TOTAL           : 220,717 frames from 635 videos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n===== CLASS FRAME SHARE =====\")\n",
        "print(f\"Real           : {real_frames / overall_frames:.2%}\")\n",
        "\n",
        "for method, path in METHOD_DIRS.items():\n",
        "    frames, _ = count_total_frames(path)\n",
        "    print(f\"{method:<15}: {frames / overall_frames:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNwnbSjGRin9",
        "outputId": "06867886-152b-4504-f6f3-09a27d93df18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== CLASS FRAME SHARE =====\n",
            "Real           : 13.95%\n",
            "Deepfakes      : 21.79%\n",
            "Face2Face      : 29.04%\n",
            "FaceSwap       : 23.01%\n",
            "NeuralTextures : 12.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 5-class label mapping (Deepfakes INCLUDED)\n",
        "CLASS_NAMES = [\"Real\", \"Deepfakes\", \"Face2Face\", \"FaceSwap\", \"NeuralTextures\"]\n",
        "CLASS_LABELS = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "print(\"CLASS_LABELS:\", CLASS_LABELS)\n",
        "\n",
        "def collect_frames(max_frames_per_video=40, seed=42):\n",
        "    \"\"\"\n",
        "    Build frame-level dataframe.\n",
        "    - Includes Deepfakes (as long as it's in METHOD_DIRS)\n",
        "    - Limits each video to `max_frames_per_video` frames so no video overwhelms the dataset\n",
        "    \"\"\"\n",
        "\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rows = []\n",
        "\n",
        "    # ---- Real frames ----\n",
        "    for video_dir in ORIGINAL_DIR.iterdir():\n",
        "        if not video_dir.is_dir():\n",
        "            continue\n",
        "\n",
        "        video_id = video_dir.name\n",
        "        frame_paths = sorted(video_dir.glob(\"*.png\"))\n",
        "\n",
        "        # Subsample frames\n",
        "        if len(frame_paths) > max_frames_per_video:\n",
        "            idxs = np.linspace(0, len(frame_paths) - 1, max_frames_per_video).astype(int)\n",
        "            frame_paths = [frame_paths[i] for i in idxs]\n",
        "\n",
        "        for fp in frame_paths:\n",
        "            rows.append({\n",
        "                \"frame_path\": str(fp),\n",
        "                \"video_id\": video_id,\n",
        "                \"class_name\": \"Real\",\n",
        "                \"class_label\": CLASS_LABELS[\"Real\"],\n",
        "            })\n",
        "\n",
        "    # ---- Manipulated frames (methods in METHOD_DIRS, incl. Deepfakes) ----\n",
        "    for method_name, method_dir in METHOD_DIRS.items():\n",
        "        for video_dir in method_dir.iterdir():\n",
        "            if not video_dir.is_dir():\n",
        "                continue\n",
        "\n",
        "            video_id = video_dir.name\n",
        "            frame_paths = sorted(video_dir.glob(\"*.png\"))\n",
        "\n",
        "            if len(frame_paths) > max_frames_per_video:\n",
        "                idxs = np.linspace(0, len(frame_paths) - 1, max_frames_per_video).astype(int)\n",
        "                frame_paths = [frame_paths[i] for i in idxs]\n",
        "\n",
        "            for fp in frame_paths:\n",
        "                rows.append({\n",
        "                    \"frame_path\": str(fp),\n",
        "                    \"video_id\": video_id,\n",
        "                    \"class_name\": method_name,\n",
        "                    \"class_label\": CLASS_LABELS[method_name],\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n",
        "\n",
        "# Build the dataframe\n",
        "df_frames = collect_frames()\n",
        "print(\"Total frames:\", len(df_frames))\n",
        "print(df_frames.head())\n",
        "\n",
        "# Quick sanity check\n",
        "print(\"\\nUnique classes found:\", df_frames[\"class_name\"].unique())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uBBuf2TT1_v",
        "outputId": "fbfab5b0-81f7-4337-8d61-053462d154c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLASS_LABELS: {'Real': 0, 'Deepfakes': 1, 'Face2Face': 2, 'FaceSwap': 3, 'NeuralTextures': 4}\n",
            "Total frames: 24480\n",
            "                                          frame_path video_id class_name  \\\n",
            "0  /content/drive/MyDrive/forensicbind/original_s...  850.mp4       Real   \n",
            "1  /content/drive/MyDrive/forensicbind/original_s...  850.mp4       Real   \n",
            "2  /content/drive/MyDrive/forensicbind/original_s...  850.mp4       Real   \n",
            "3  /content/drive/MyDrive/forensicbind/original_s...  850.mp4       Real   \n",
            "4  /content/drive/MyDrive/forensicbind/original_s...  850.mp4       Real   \n",
            "\n",
            "   class_label  \n",
            "0            0  \n",
            "1            0  \n",
            "2            0  \n",
            "3            0  \n",
            "4            0  \n",
            "\n",
            "Unique classes found: ['Real' 'Deepfakes' 'Face2Face' 'FaceSwap' 'NeuralTextures']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# df_frames columns assumed:\n",
        "# frame_path, video_id, class_name, class_label\n",
        "\n",
        "# 0) Make a unique per-video key to avoid collisions across methods\n",
        "# (FF++ reuses names like 000_003.mp4 across different manipulation folders)\n",
        "df_frames = df_frames.copy()\n",
        "df_frames[\"video_key\"] = df_frames[\"class_name\"].astype(str) + \"___\" + df_frames[\"video_id\"].astype(str)\n",
        "\n",
        "# 1) Build a VIDEO-level table for splitting (one row per video_key)\n",
        "df_videos = (\n",
        "    df_frames\n",
        "    .groupby([\"video_key\", \"video_id\", \"class_name\", \"class_label\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"num_frames\")\n",
        ")\n",
        "\n",
        "print(\"Videos per class:\")\n",
        "print(df_videos[\"class_name\"].value_counts())\n",
        "\n",
        "# Optional: drop tiny videos if you want\n",
        "MIN_FRAMES = 1\n",
        "df_videos = df_videos[df_videos[\"num_frames\"] >= MIN_FRAMES].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nVideos per class after MIN_FRAMES>={MIN_FRAMES}:\")\n",
        "print(df_videos[\"class_name\"].value_counts())\n",
        "\n",
        "# 2) Stratified train/val/test split at VIDEO level\n",
        "train_ratio = 0.7\n",
        "val_ratio   = 0.15\n",
        "test_ratio  = 0.15\n",
        "\n",
        "train_videos, temp_videos = train_test_split(\n",
        "    df_videos,\n",
        "    test_size=(1 - train_ratio),\n",
        "    stratify=df_videos[\"class_label\"],\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "relative_val_ratio = val_ratio / (val_ratio + test_ratio)\n",
        "val_videos, test_videos = train_test_split(\n",
        "    temp_videos,\n",
        "    test_size=(1 - relative_val_ratio),\n",
        "    stratify=temp_videos[\"class_label\"],\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"\\nTrain videos per class:\")\n",
        "print(train_videos[\"class_name\"].value_counts())\n",
        "print(\"\\nVal videos per class:\")\n",
        "print(val_videos[\"class_name\"].value_counts())\n",
        "print(\"\\nTest videos per class:\")\n",
        "print(test_videos[\"class_name\"].value_counts())\n",
        "\n",
        "# 3) Expand splits back to FRAME level\n",
        "train_keys = set(train_videos[\"video_key\"])\n",
        "val_keys   = set(val_videos[\"video_key\"])\n",
        "test_keys  = set(test_videos[\"video_key\"])\n",
        "\n",
        "df_frames[\"split\"] = \"none\"\n",
        "df_frames.loc[df_frames[\"video_key\"].isin(train_keys), \"split\"] = \"train\"\n",
        "df_frames.loc[df_frames[\"video_key\"].isin(val_keys),   \"split\"] = \"val\"\n",
        "df_frames.loc[df_frames[\"video_key\"].isin(test_keys),  \"split\"] = \"test\"\n",
        "\n",
        "df_train = df_frames[df_frames[\"split\"] == \"train\"].copy()\n",
        "df_val   = df_frames[df_frames[\"split\"] == \"val\"].copy()\n",
        "df_test  = df_frames[df_frames[\"split\"] == \"test\"].copy()\n",
        "\n",
        "# 4) Save CSVs\n",
        "OUTPUT_DIR = DATA_ROOT / \"splits\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df_train.to_csv(OUTPUT_DIR / \"train.csv\", index=False)\n",
        "df_val.to_csv(OUTPUT_DIR / \"val.csv\", index=False)\n",
        "df_test.to_csv(OUTPUT_DIR / \"test.csv\", index=False)\n",
        "\n",
        "print(\"\\nSaved splits to:\", OUTPUT_DIR)\n",
        "print(\"Train frames:\", len(df_train))\n",
        "print(\"Val frames:\", len(df_val))\n",
        "print(\"Test frames:\", len(df_test))\n",
        "\n",
        "print(\"\\nFrame distribution (train):\")\n",
        "print(df_train[\"class_name\"].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOK6dqotUGIW",
        "outputId": "c50f9d28-37bc-4399-8f59-9bbab227d38c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Videos per class:\n",
            "class_name\n",
            "Real              200\n",
            "FaceSwap          132\n",
            "Face2Face         129\n",
            "Deepfakes         100\n",
            "NeuralTextures     71\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Videos per class after MIN_FRAMES>=1:\n",
            "class_name\n",
            "Real              200\n",
            "FaceSwap          132\n",
            "Face2Face         129\n",
            "Deepfakes         100\n",
            "NeuralTextures     71\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train videos per class:\n",
            "class_name\n",
            "Real              140\n",
            "FaceSwap           92\n",
            "Face2Face          90\n",
            "Deepfakes          70\n",
            "NeuralTextures     50\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Val videos per class:\n",
            "class_name\n",
            "Real              30\n",
            "FaceSwap          20\n",
            "Face2Face         19\n",
            "Deepfakes         15\n",
            "NeuralTextures    11\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test videos per class:\n",
            "class_name\n",
            "Real              30\n",
            "Face2Face         20\n",
            "FaceSwap          20\n",
            "Deepfakes         15\n",
            "NeuralTextures    10\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Saved splits to: /content/drive/MyDrive/forensicbind/splits\n",
            "Train frames: 17065\n",
            "Val frames: 3762\n",
            "Test frames: 3653\n",
            "\n",
            "Frame distribution (train):\n",
            "class_name\n",
            "Real              4985\n",
            "FaceSwap          3680\n",
            "Face2Face         3600\n",
            "Deepfakes         2800\n",
            "NeuralTextures    2000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 5-class mapping (must match CLASS_LABELS / CLASS_NAMES exactly)\n",
        "class_map = {\n",
        "    0: \"Real\",\n",
        "    1: \"Deepfakes\",\n",
        "    2: \"Face2Face\",\n",
        "    3: \"FaceSwap\",\n",
        "    4: \"NeuralTextures\",\n",
        "}\n",
        "\n",
        "CLASS_MAP_PATH = DATA_ROOT / \"class_map.json\"\n",
        "with open(CLASS_MAP_PATH, \"w\") as f:\n",
        "    json.dump(class_map, f, indent=2)\n",
        "\n",
        "print(\"Saved class map to:\", CLASS_MAP_PATH)\n",
        "class_map\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ5sIMPOU1IC",
        "outputId": "eef6aecc-6bb1-4d0d-9d7b-2ccab695b4d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved class map to: /content/drive/MyDrive/forensicbind/class_map.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Real', 1: 'Deepfakes', 2: 'Face2Face', 3: 'FaceSwap', 4: 'NeuralTextures'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU count:\", torch.cuda.device_count())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kVQEPjMU84w",
        "outputId": "40b01de9-688a-4501-a7b5-69e0a7a3d90f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU count: 1\n",
            "Using GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n"
      ],
      "metadata": {
        "id": "Cws6cXIDVFjk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SPLITS_DIR = DATA_ROOT / \"splits\"\n",
        "\n",
        "print(\"Data root:\", DATA_ROOT)\n",
        "print(\"Splits:\", list(SPLITS_DIR.glob(\"*.csv\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcGmFs0XVJMw",
        "outputId": "a653e50f-6a42-43e0-9853-8816da1d4ee7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data root: /content/drive/MyDrive/forensicbind\n",
            "Splits: [PosixPath('/content/drive/MyDrive/forensicbind/splits/train.csv'), PosixPath('/content/drive/MyDrive/forensicbind/splits/val.csv'), PosixPath('/content/drive/MyDrive/forensicbind/splits/test.csv')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "\n",
        "# Use ImageNet mean/std for timm Vision Transformers\n",
        "VIT_MEAN = [0.485, 0.456, 0.406]\n",
        "VIT_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "print(\"IMG_SIZE:\", IMG_SIZE)\n",
        "print(\"VIT_MEAN:\", VIT_MEAN)\n",
        "print(\"VIT_STD:\", VIT_STD)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umAFxQq9VNXX",
        "outputId": "8a600546-711f-4431-e4f3-ba922d264768"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMG_SIZE: 224\n",
            "VIT_MEAN: [0.485, 0.456, 0.406]\n",
            "VIT_STD: [0.229, 0.224, 0.225]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomJPEGCompression:\n",
        "    \"\"\"\n",
        "    Mild JPEG compression for augmentation.\n",
        "    Used to simulate realistic compression while preserving\n",
        "    manipulation artifacts that are important for attribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, quality_range=(70, 100), p=0.3):\n",
        "        self.quality_range = quality_range\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img: Image.Image) -> Image.Image:\n",
        "        if random.random() > self.p:\n",
        "            return img\n",
        "\n",
        "        buffer = io.BytesIO()\n",
        "        quality = random.randint(*self.quality_range)\n",
        "        img.save(buffer, format=\"JPEG\", quality=quality)\n",
        "        buffer.seek(0)\n",
        "        compressed = Image.open(buffer).convert(\"RGB\")\n",
        "        return compressed\n"
      ],
      "metadata": {
        "id": "cMmPa8pyVR_s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_transform():\n",
        "    \"\"\"\n",
        "    Softer, forensic-safe augmentations for SupCon.\n",
        "    These preserve manipulation artifacts while still providing\n",
        "    meaningful variation for contrastive learning.\n",
        "    \"\"\"\n",
        "    return T.Compose([\n",
        "        # Less aggressive cropping\n",
        "        T.RandomResizedCrop(IMG_SIZE, scale=(0.9, 1.0)),\n",
        "\n",
        "        T.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "        # Light color jitter (strong jitter destroys forensic cues)\n",
        "        T.ColorJitter(\n",
        "            brightness=0.1,\n",
        "            contrast=0.1,\n",
        "            saturation=0.1,\n",
        "            hue=0.02,\n",
        "        ),\n",
        "\n",
        "        # Very mild blur\n",
        "        T.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5)),\n",
        "\n",
        "        # Mild JPEG compression to simulate camera artifacts\n",
        "        RandomJPEGCompression(quality_range=(70, 100), p=0.3),\n",
        "\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=VIT_MEAN, std=VIT_STD),\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "Im9SBYkwVarH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_eval_transform():\n",
        "    \"\"\"\n",
        "    Evaluation / test transform.\n",
        "    No heavy augmentation — just resize + normalize to ImageNet stats.\n",
        "    Used for:\n",
        "      - SupCon validation\n",
        "      - Linear probe train/val/test\n",
        "    \"\"\"\n",
        "    return T.Compose([\n",
        "        T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=VIT_MEAN, std=VIT_STD),\n",
        "    ])"
      ],
      "metadata": {
        "id": "6vfSzVP8Vg19"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SupConImageDataset(Dataset):\n",
        "    def __init__(self, csv_path, transform=None, two_views=True, return_path=False):\n",
        "        \"\"\"\n",
        "        csv_path: path to the split CSV (train/val/test)\n",
        "        transform: torchvision transform to apply\n",
        "        two_views: if True -> (x1, x2, label), else (x, label)\n",
        "        return_path: if True -> also return frame_path for debugging\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.transform = transform\n",
        "        self.two_views = two_views\n",
        "        self.return_path = return_path\n",
        "\n",
        "        # Basic sanity checks\n",
        "        assert \"frame_path\" in self.df.columns, \"CSV must contain 'frame_path'\"\n",
        "        assert \"class_label\" in self.df.columns, \"CSV must contain 'class_label'\"\n",
        "        assert \"class_name\" in self.df.columns, \"CSV must contain 'class_name'\"\n",
        "\n",
        "        # ✅ Updated sanity check for 5-class setup\n",
        "        unique_labels = sorted(self.df[\"class_label\"].unique())\n",
        "        expected_labels = set(range(NUM_CLASSES))  # NUM_CLASSES = 5\n",
        "        assert set(unique_labels).issubset(expected_labels), \\\n",
        "            f\"Unexpected labels {unique_labels}. Expected subset of {expected_labels}\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = row[\"frame_path\"]\n",
        "        label = int(row[\"class_label\"])\n",
        "\n",
        "        # Load image\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            if self.two_views:\n",
        "                x1 = self.transform(img)\n",
        "                x2 = self.transform(img)\n",
        "                if self.return_path:\n",
        "                    return x1, x2, label, img_path\n",
        "                return x1, x2, label\n",
        "            else:\n",
        "                x = self.transform(img)\n",
        "                if self.return_path:\n",
        "                    return x, label, img_path\n",
        "                return x, label\n",
        "        else:\n",
        "            # Fallback: minimal behavior\n",
        "            x = T.ToTensor()(img)\n",
        "            if self.two_views:\n",
        "                if self.return_path:\n",
        "                    return x, x.clone(), label, img_path\n",
        "                return x, x.clone(), label\n",
        "            else:\n",
        "                if self.return_path:\n",
        "                    return x, label, img_path\n",
        "                return x, label\n"
      ],
      "metadata": {
        "id": "bNyPeh7qVthD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "\n",
        "def get_dataloaders(\n",
        "    data_root: Path,\n",
        "    batch_size: int = 32,\n",
        "    num_workers: int = 4,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns dataloaders for SupCon training:\n",
        "      - train: two views (x1, x2, label) with imbalance-aware sampling\n",
        "      - val:   single view\n",
        "      - test:  single view\n",
        "    \"\"\"\n",
        "\n",
        "    splits_dir = data_root / \"splits\"\n",
        "    train_csv = splits_dir / \"train.csv\"\n",
        "    val_csv   = splits_dir / \"val.csv\"\n",
        "    test_csv  = splits_dir / \"test.csv\"\n",
        "\n",
        "    assert train_csv.exists(), f\"train.csv not found at {train_csv}\"\n",
        "    assert val_csv.exists(),   f\"val.csv not found at {val_csv}\"\n",
        "    assert test_csv.exists(),  f\"test.csv not found at {test_csv}\"\n",
        "\n",
        "    train_transform = get_train_transform()\n",
        "    eval_transform  = get_eval_transform()\n",
        "\n",
        "    # ---- SupCon training: 2 augmented views ----\n",
        "    train_dataset = SupConImageDataset(\n",
        "        csv_path=train_csv,\n",
        "        transform=train_transform,\n",
        "        two_views=True,\n",
        "    )\n",
        "\n",
        "    # ---- Validation/Test: single clean view ----\n",
        "    val_dataset = SupConImageDataset(\n",
        "        csv_path=val_csv,\n",
        "        transform=eval_transform,\n",
        "        two_views=False,\n",
        "    )\n",
        "\n",
        "    test_dataset = SupConImageDataset(\n",
        "        csv_path=test_csv,\n",
        "        transform=eval_transform,\n",
        "        two_views=False,\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # Weighted sampling for TRAIN (handles class imbalance)\n",
        "    # -------------------------------------------------\n",
        "    # Per-sample weights based on class frequency in the TRAIN split\n",
        "    labels = train_dataset.df[\"class_label\"].values\n",
        "    class_counts = train_dataset.df[\"class_label\"].value_counts().sort_index()\n",
        "    class_weights = 1.0 / class_counts\n",
        "    sample_weights = class_weights[labels].values\n",
        "\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=torch.as_tensor(sample_weights, dtype=torch.double),\n",
        "        num_samples=len(sample_weights),\n",
        "        replacement=True,\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # DataLoaders\n",
        "    # -------------------------------------------------\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        sampler=sampler,     # ✅ replaces shuffle=True\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,      # helpful for consistent contrastive batches\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ],
      "metadata": {
        "id": "op_rB_clVu-H"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_workers = 4  # reduce to 2 if Colab complains\n",
        "\n",
        "train_loader, val_loader, test_loader = get_dataloaders(\n",
        "    DATA_ROOT,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4KJ5UGqV5S-",
        "outputId": "36d2ca0b-e4e0-47ac-aac2-851e839957d4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))\n",
        "\n",
        "# train_loader uses two_views=True → batch = (x1, x2, labels)\n",
        "if isinstance(batch, (list, tuple)) and len(batch) == 3:\n",
        "    x1, x2, labels = batch\n",
        "    print(\"Two-view batch (SupCon mode):\")\n",
        "    print(\"  x1 shape:\", x1.shape)\n",
        "    print(\"  x2 shape:\", x2.shape)\n",
        "    print(\"  labels shape:\", labels.shape)\n",
        "    print(\"  labels[:10] =\", labels[:10].tolist())\n",
        "\n",
        "# If you ever use eval loaders here:\n",
        "elif isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
        "    x, labels = batch\n",
        "    print(\"Single-view batch (eval/linear-probe mode):\")\n",
        "    print(\"  x shape:\", x.shape)\n",
        "    print(\"  labels shape:\", labels.shape)\n",
        "    print(\"  labels[:10] =\", labels[:10].tolist())\n",
        "\n",
        "else:\n",
        "    print(\"Unexpected batch structure:\", type(batch), batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxoMxOlgV_Qx",
        "outputId": "2c10f599-a732-48fe-d933-f788b2fc9fee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two-view batch (SupCon mode):\n",
            "  x1 shape: torch.Size([32, 3, 224, 224])\n",
            "  x2 shape: torch.Size([32, 3, 224, 224])\n",
            "  labels shape: torch.Size([32])\n",
            "  labels[:10] = [0, 2, 4, 1, 0, 0, 3, 1, 3, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "for i, batch in enumerate(train_loader):\n",
        "\n",
        "    # -------------------------\n",
        "    # SupCon mode → (x1, x2, labels)\n",
        "    # -------------------------\n",
        "    if isinstance(batch, (list, tuple)) and len(batch) == 3:\n",
        "        x1, x2, labels = batch\n",
        "        x1 = x1.to(device, non_blocking=True)\n",
        "        x2 = x2.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        print(f\"Batch {i} (SupCon, 2-views)\")\n",
        "        print(\"  x1 shape:\", x1.shape)\n",
        "        print(\"  x2 shape:\", x2.shape)\n",
        "        print(\"  labels shape:\", labels.shape)\n",
        "        print(\"  unique labels:\", labels.unique().tolist())\n",
        "\n",
        "    # -------------------------\n",
        "    # Eval / Linear-Probe mode → (x, labels)\n",
        "    # -------------------------\n",
        "    elif isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
        "        x, labels = batch\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        print(f\"Batch {i} (Eval/Probe, 1-view)\")\n",
        "        print(\"  x shape:\", x.shape)\n",
        "        print(\"  labels shape:\", labels.shape)\n",
        "        print(\"  unique labels:\", labels.unique().tolist())\n",
        "\n",
        "    else:\n",
        "        print(\"Unexpected batch format:\", type(batch), batch)\n",
        "\n",
        "    # limit preview\n",
        "    if i == 2:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXmZWAd3WDro",
        "outputId": "7643b243-4a65-4c31-e0de-b2d217394bb4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 (SupCon, 2-views)\n",
            "  x1 shape: torch.Size([32, 3, 224, 224])\n",
            "  x2 shape: torch.Size([32, 3, 224, 224])\n",
            "  labels shape: torch.Size([32])\n",
            "  unique labels: [0, 1, 2, 3, 4]\n",
            "Batch 1 (SupCon, 2-views)\n",
            "  x1 shape: torch.Size([32, 3, 224, 224])\n",
            "  x2 shape: torch.Size([32, 3, 224, 224])\n",
            "  labels shape: torch.Size([32])\n",
            "  unique labels: [0, 1, 2, 3, 4]\n",
            "Batch 2 (SupCon, 2-views)\n",
            "  x1 shape: torch.Size([32, 3, 224, 224])\n",
            "  x2 shape: torch.Size([32, 3, 224, 224])\n",
            "  labels shape: torch.Size([32])\n",
            "  unique labels: [0, 1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 3"
      ],
      "metadata": {
        "id": "NnMoNEHnWHc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "import timm\n"
      ],
      "metadata": {
        "id": "cdhcwU3KWJm0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "# Auto-detect GPU\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# Use Drive-backed project root (consistent with rest of notebook)\n",
        "PROJECT_ROOT = DATA_ROOT\n",
        "\n",
        "# Create checkpoint directory if missing\n",
        "CHECKPOINT_DIR = PROJECT_ROOT / \"checkpoints\"\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Checkpoint directory:\", CHECKPOINT_DIR.resolve())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9S6jXGuWQsq",
        "outputId": "acd88539-d159-48f9-93dd-d024a3227f9a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Checkpoint directory: /content/drive/MyDrive/forensicbind/checkpoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer encoder wrapper.\n",
        "    - Loads a timm ViT with num_classes=0 → returns CLS embedding.\n",
        "    - Stores embedding dimension for projection head & classifier.\n",
        "    - Allows selective fine-tuning of last N transformer blocks.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name=\"vit_base_patch16_224\", pretrained=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create ViT backbone without classification head\n",
        "        self.backbone = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            num_classes=0   # return CLS embedding (pooled)\n",
        "        )\n",
        "\n",
        "        # CLS embedding dimension (e.g., 768 for base models)\n",
        "        self.embed_dim = getattr(self.backbone, \"num_features\", 768)\n",
        "\n",
        "        # Get depth safely (vit_base has 12, but other models may differ)\n",
        "        self.depth = len(getattr(self.backbone, \"blocks\", [])) if hasattr(self.backbone, \"blocks\") else 12\n",
        "\n",
        "    def freeze_all(self):\n",
        "        \"\"\"Freeze entire encoder (used before linear-probe training).\"\"\"\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def unfreeze_last_blocks(self, num_blocks=2):\n",
        "        \"\"\"\n",
        "        Unfreeze last N transformer blocks + final norm.\n",
        "        Used after linear-probe training for fine-tuning.\n",
        "        \"\"\"\n",
        "        self.freeze_all()\n",
        "\n",
        "        start = max(0, self.depth - num_blocks)\n",
        "\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            # Unfreeze last N blocks\n",
        "            if any(f\"blocks.{i}.\" in name for i in range(start, self.depth)):\n",
        "                param.requires_grad = True\n",
        "            # Also unfreeze final norm\n",
        "            if \"norm\" in name:\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass returns CLS embedding vector.\n",
        "        Shape: [B, embed_dim]\n",
        "        \"\"\"\n",
        "        feats = self.backbone(x)\n",
        "        return feats\n"
      ],
      "metadata": {
        "id": "8VLK4sDdWvfp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = ViTEncoder().to(DEVICE)\n",
        "\n",
        "print(\"========== ViT Encoder ==========\")\n",
        "print(encoder)\n",
        "\n",
        "# Print embedding dimension for clarity\n",
        "print(\"\\nEncoder embedding dimension:\", encoder.embed_dim)\n",
        "\n",
        "# Parameter count summary\n",
        "total_params = sum(p.numel() for p in encoder.parameters())\n",
        "trainable_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total parameters:     {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(\"Model is on device:\", next(encoder.parameters()).device)\n",
        "print(\"=================================\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f4211eba8887472195944f0a28dda293",
            "7aa511c15bec492ab3e028972f6e07ce",
            "f656f0fc9a6f46009bee9ae242ed06c1",
            "ed293ba6e3224ae68629e91748c00b53",
            "9ea20f4bd602445c88fb6948c27bcc28",
            "7961c27313e1438885417ebe05c02526",
            "0dc381f981cb491eabb764674e20fb5e",
            "4ea623a59914494ca4811a52e7f21eff",
            "ebd04f4176b34748ba93f63b29ee0590",
            "c07965671b7f40f1b215a9101429763a",
            "2d647a99308145d582d1d632a1fd8e90"
          ]
        },
        "id": "D2zjtFgZWxp3",
        "outputId": "53f02ba3-09bb-4a07-fd9f-e67d8c4b780d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4211eba8887472195944f0a28dda293"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== ViT Encoder ==========\n",
            "ViTEncoder(\n",
            "  (backbone): VisionTransformer(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      (norm): Identity()\n",
            "    )\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (patch_drop): Identity()\n",
            "    (norm_pre): Identity()\n",
            "    (blocks): Sequential(\n",
            "      (0): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (q_norm): Identity()\n",
            "          (k_norm): Identity()\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls1): Identity()\n",
            "        (drop_path1): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls2): Identity()\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (1): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (q_norm): Identity()\n",
            "          (k_norm): Identity()\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls1): Identity()\n",
            "        (drop_path1): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls2): Identity()\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (2): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (q_norm): Identity()\n",
            "          (k_norm): Identity()\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls1): Identity()\n",
            "        (drop_path1): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls2): Identity()\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (3): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (q_norm): Identity()\n",
            "          (k_norm): Identity()\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls1): Identity()\n",
            "        (drop_path1): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls2): Identity()\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (4): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (q_norm): Identity()\n",
            "          (k_norm): Identity()\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls1): Identity()\n",
            "        (drop_path1): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls2): Identity()\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (5): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (q_norm): Identity()\n",
            "          (k_norm): Identity()\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls1): Identity()\n",
            "        (drop_path1): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls2): Identity()\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (6): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (q_norm): Identity()\n",
            "          (k_norm): Identity()\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls1): Identity()\n",
            "        (drop_path1): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls2): Identity()\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (7): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (q_norm): Identity()\n",
            "          (k_norm): Identity()\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls1): Identity()\n",
            "        (drop_path1): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls2): Identity()\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (8): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (q_norm): Identity()\n",
            "          (k_norm): Identity()\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls1): Identity()\n",
            "        (drop_path1): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls2): Identity()\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (9): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (q_norm): Identity()\n",
            "          (k_norm): Identity()\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls1): Identity()\n",
            "        (drop_path1): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls2): Identity()\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (10): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (q_norm): Identity()\n",
            "          (k_norm): Identity()\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls1): Identity()\n",
            "        (drop_path1): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls2): Identity()\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "      (11): Block(\n",
            "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (q_norm): Identity()\n",
            "          (k_norm): Identity()\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls1): Identity()\n",
            "        (drop_path1): Identity()\n",
            "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (drop1): Dropout(p=0.0, inplace=False)\n",
            "          (norm): Identity()\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (drop2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ls2): Identity()\n",
            "        (drop_path2): Identity()\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    (fc_norm): Identity()\n",
            "    (head_drop): Dropout(p=0.0, inplace=False)\n",
            "    (head): Identity()\n",
            "  )\n",
            ")\n",
            "\n",
            "Encoder embedding dimension: 768\n",
            "Total parameters:     85,798,656\n",
            "Trainable parameters: 85,798,656\n",
            "Model is on device: cuda:0\n",
            "=================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    2-layer projection head for SupCon (from the official paper).\n",
        "    Input:  encoder CLS embedding (dim = encoder.embed_dim)\n",
        "    Output: normalized projection vector (out_dim)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, hidden_dim=2048, out_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Linear(hidden_dim, out_dim),\n",
        "            nn.BatchNorm1d(out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.net(x)            # [B, out_dim]\n",
        "        z = F.normalize(z, dim=1)  # L2 normalize\n",
        "        return z\n",
        "\n",
        "\n",
        "projection_head = ProjectionHead(\n",
        "    in_dim=encoder.embed_dim,\n",
        "    hidden_dim=2048,\n",
        "    out_dim=128,\n",
        ").to(DEVICE)\n",
        "\n",
        "print(\"========== Projection Head ==========\")\n",
        "print(projection_head)\n",
        "print(\"Input dim:\", encoder.embed_dim)\n",
        "print(\"====================================\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg2S8hq3W7NY",
        "outputId": "53171269-d660-4660-fe3a-8fae80996c3f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== Projection Head ==========\n",
            "ProjectionHead(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=2048, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): Linear(in_features=2048, out_features=128, bias=True)\n",
            "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "Input dim: 768\n",
            "====================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SupConLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Supervised Contrastive Loss (Khosla et al.)\n",
        "\n",
        "    IMPORTANT:\n",
        "      - This implementation expects:\n",
        "          features: [N, dim] where N = batch_size * n_views  (e.g., 2B)\n",
        "          labels:   [N] with labels repeated for each view (e.g., [2B])\n",
        "      - If your dataloader returns labels as [B], repeat them before calling:\n",
        "          labels = torch.cat([labels, labels], dim=0)   # if features = cat([f1, f2], dim=0)\n",
        "    \"\"\"\n",
        "    def __init__(self, temperature=0.07):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, features, labels):\n",
        "        \"\"\"\n",
        "        features: tensor of shape [N, dim]\n",
        "        labels:   tensor of shape [N]\n",
        "        \"\"\"\n",
        "        device = features.device\n",
        "\n",
        "        # Normalize (projection head already normalizes, but this is safe)\n",
        "        features = F.normalize(features, dim=1)\n",
        "\n",
        "        N = features.size(0)\n",
        "\n",
        "        # [N, 1]\n",
        "        labels = labels.contiguous().view(-1, 1)\n",
        "\n",
        "        # Positive-pair mask: 1 if same label, 0 otherwise\n",
        "        mask = torch.eq(labels, labels.T).float().to(device)\n",
        "\n",
        "        # Remove self-comparisons\n",
        "        self_mask = torch.eye(N, dtype=torch.float32, device=device)\n",
        "        mask = mask - self_mask  # positives only for i != j\n",
        "\n",
        "        # Similarity matrix / temperature\n",
        "        logits = torch.div(features @ features.T, self.temperature)\n",
        "\n",
        "        # Numerical stability\n",
        "        logits_max, _ = torch.max(logits, dim=1, keepdim=True)\n",
        "        logits = logits - logits_max.detach()\n",
        "\n",
        "        # Mask out diagonal in denominator\n",
        "        logits_mask = 1.0 - self_mask\n",
        "        exp_logits = torch.exp(logits) * logits_mask\n",
        "\n",
        "        # Log-probabilities\n",
        "        log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True) + 1e-12)\n",
        "\n",
        "        # Average over positives for each anchor\n",
        "        pos_mask_sum = mask.sum(dim=1)\n",
        "        valid = pos_mask_sum > 0\n",
        "\n",
        "        log_prob_pos = (mask * log_prob).sum(dim=1) / (pos_mask_sum + 1e-12)\n",
        "\n",
        "        # Final loss\n",
        "        if valid.any():\n",
        "            loss = -log_prob_pos[valid].mean()\n",
        "        else:\n",
        "            loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "iMyOnFlvXFnc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SupCon loss (temperature from paper)\n",
        "supcon_temperature = 0.07\n",
        "supcon_criterion = SupConLoss(temperature=supcon_temperature).to(DEVICE)\n",
        "\n",
        "print(f\"SupCon temperature: {supcon_temperature}\")\n",
        "print(\"SupConLoss device:\", next(supcon_criterion.parameters(), torch.tensor(0., device=DEVICE)).device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUmCQwelXXt7",
        "outputId": "a6356f7b-9800-453b-aff5-376ae4eb6865"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SupCon temperature: 0.07\n",
            "SupConLoss device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# SupCon Training Hyperparams\n",
        "# -----------------------------\n",
        "lr = 1e-4\n",
        "weight_decay = 1e-4\n",
        "num_epochs = 10   # 10–15 is fine for testing\n",
        "\n",
        "# Encoder + Projection Head parameters\n",
        "params = list(encoder.parameters()) + list(projection_head.parameters())\n",
        "\n",
        "optimizer = AdamW(\n",
        "    params,\n",
        "    lr=lr,\n",
        "    weight_decay=weight_decay,\n",
        ")\n",
        "\n",
        "print(\"Optimizer = AdamW\")\n",
        "print(\"Learning rate:\", lr)\n",
        "print(\"Weight decay:\", weight_decay)\n",
        "print(f\"Total trainable params: {sum(p.numel() for p in params if p.requires_grad):,}\")\n",
        "\n",
        "# Optional: enable AMP for faster training\n",
        "use_amp = torch.cuda.is_available()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "print(\"AMP enabled:\", use_amp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxqmRWA5Xczf",
        "outputId": "82ef5910-f828-4459-e9dd-3c08d8e76f8d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer = AdamW\n",
            "Learning rate: 0.0001\n",
            "Weight decay: 0.0001\n",
            "Total trainable params: 87,640,192\n",
            "AMP enabled: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3101798744.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(\n",
        "    epoch,\n",
        "    encoder,\n",
        "    projection_head,\n",
        "    dataloader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    device,\n",
        "    scaler=None,\n",
        "    use_amp=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    One epoch of SupCon training.\n",
        "    Expects dataloader to yield (x1, x2, labels) batches.\n",
        "    \"\"\"\n",
        "    encoder.train()\n",
        "    projection_head.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    loop = tqdm(dataloader, desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
        "\n",
        "    # Use correct autocast device type\n",
        "    amp_device_type = \"cuda\" if device.type == \"cuda\" else \"cpu\"\n",
        "\n",
        "    for batch in loop:\n",
        "        x1, x2, labels = batch\n",
        "        x1 = x1.to(device, non_blocking=True)\n",
        "        x2 = x2.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # ------------------ forward ------------------\n",
        "        if use_amp and scaler is not None:\n",
        "            with torch.autocast(device_type=amp_device_type, enabled=True):\n",
        "                feats1 = encoder(x1)           # [B, D]\n",
        "                feats2 = encoder(x2)           # [B, D]\n",
        "\n",
        "                z1 = projection_head(feats1)   # [B, 128]\n",
        "                z2 = projection_head(feats2)   # [B, 128]\n",
        "\n",
        "                features = torch.cat([z1, z2], dim=0)               # [2B, 128]\n",
        "                labels_concat = torch.cat([labels, labels], dim=0)  # [2B]\n",
        "                loss = criterion(features, labels_concat)\n",
        "        else:\n",
        "            feats1 = encoder(x1)\n",
        "            feats2 = encoder(x2)\n",
        "\n",
        "            z1 = projection_head(feats1)\n",
        "            z2 = projection_head(feats2)\n",
        "\n",
        "            features = torch.cat([z1, z2], dim=0)\n",
        "            labels_concat = torch.cat([labels, labels], dim=0)\n",
        "            loss = criterion(features, labels_concat)\n",
        "\n",
        "        # ------------------ backward ------------------\n",
        "        if use_amp and scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += float(loss.item())\n",
        "        n_batches += 1\n",
        "        loop.set_postfix(loss=float(loss.item()))\n",
        "\n",
        "    avg_loss = running_loss / max(1, n_batches)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(\n",
        "    epoch,\n",
        "    encoder,\n",
        "    projection_head,\n",
        "    dataloader,\n",
        "    criterion,\n",
        "    device,\n",
        "):\n",
        "    \"\"\"\n",
        "    One epoch of SupCon validation.\n",
        "    Works with either:\n",
        "      - (x1, x2, labels)  -> 2-view batches\n",
        "      - (x, labels)       -> single-view batches\n",
        "    \"\"\"\n",
        "    encoder.eval()\n",
        "    projection_head.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    loop = tqdm(dataloader, desc=f\"Epoch {epoch} [Val]\", leave=False)\n",
        "\n",
        "    for batch in loop:\n",
        "        if isinstance(batch, (list, tuple)) and len(batch) == 3:\n",
        "            x1, x2, labels = batch\n",
        "            x1 = x1.to(device, non_blocking=True)\n",
        "            x2 = x2.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            feats1 = encoder(x1)\n",
        "            feats2 = encoder(x2)\n",
        "            z1 = projection_head(feats1)\n",
        "            z2 = projection_head(feats2)\n",
        "\n",
        "            features = torch.cat([z1, z2], dim=0)\n",
        "            labels_concat = torch.cat([labels, labels], dim=0)\n",
        "\n",
        "        elif isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
        "            x, labels = batch\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            feats = encoder(x)\n",
        "            features = projection_head(feats)   # [B, 128]\n",
        "            labels_concat = labels              # [B]\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected batch format in eval: type={type(batch)}\")\n",
        "\n",
        "        loss = criterion(features, labels_concat)\n",
        "\n",
        "        running_loss += float(loss.item())\n",
        "        n_batches += 1\n",
        "        loop.set_postfix(loss=float(loss.item()))\n",
        "\n",
        "    avg_loss = running_loss / max(1, n_batches)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "QCQq49fRXtpK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "best_epoch = -1\n",
        "\n",
        "history = {\n",
        "    \"train_loss\": [],\n",
        "    \"val_loss\": [],\n",
        "}\n",
        "\n",
        "print(\"Starting SupCon training...\\n\")\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # -------- Train --------\n",
        "    train_loss = train_one_epoch(\n",
        "        epoch=epoch,\n",
        "        encoder=encoder,\n",
        "        projection_head=projection_head,\n",
        "        dataloader=train_loader,\n",
        "        optimizer=optimizer,\n",
        "        criterion=supcon_criterion,\n",
        "        device=DEVICE,\n",
        "        scaler=scaler,\n",
        "        use_amp=use_amp,\n",
        "    )\n",
        "\n",
        "    # -------- Validate --------\n",
        "    val_loss = eval_one_epoch(\n",
        "        epoch=epoch,\n",
        "        encoder=encoder,\n",
        "        projection_head=projection_head,\n",
        "        dataloader=val_loader,\n",
        "        criterion=supcon_criterion,\n",
        "        device=DEVICE,\n",
        "    )\n",
        "\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:03d} | \"\n",
        "        f\"train_loss = {train_loss:.4f} | \"\n",
        "        f\"val_loss = {val_loss:.4f}\"\n",
        "    )\n",
        "\n",
        "    # -------- Save last checkpoint --------\n",
        "    last_ckpt_path = CHECKPOINT_DIR / \"supcon_last.pth\"\n",
        "    ckpt = {\n",
        "        \"epoch\": epoch,\n",
        "        \"encoder_state_dict\": encoder.state_dict(),\n",
        "        \"projection_head_state_dict\": projection_head.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"train_loss\": train_loss,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"use_amp\": use_amp,\n",
        "    }\n",
        "    if use_amp and scaler is not None:\n",
        "        ckpt[\"scaler_state_dict\"] = scaler.state_dict()\n",
        "\n",
        "    torch.save(ckpt, last_ckpt_path)\n",
        "\n",
        "    # -------- Save best based on val loss --------\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "\n",
        "        torch.save(encoder.state_dict(), CHECKPOINT_DIR / \"encoder_supcon.pth\")\n",
        "        torch.save(projection_head.state_dict(), CHECKPOINT_DIR / \"projection_head_supcon.pth\")\n",
        "\n",
        "        print(f\"  🔥 New best model at epoch {epoch}, val_loss = {val_loss:.4f}\")\n",
        "        print(f\"  Saved encoder_supcon.pth and projection_head_supcon.pth to {CHECKPOINT_DIR}\")\n",
        "\n",
        "print(\"\\nSupCon training complete.\")\n",
        "print(\"Best val loss:\", best_val_loss, \"at epoch\", best_epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYA2IifUXvMQ",
        "outputId": "6f2c5272-c125-4d4e-cee0-5216a331fc5d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting SupCon training...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1 [Train]:   0%|          | 0/533 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss = 5.7294 | val_loss = 4.7088\n",
            "  🔥 New best model at epoch 1, val_loss = 4.7088\n",
            "  Saved encoder_supcon.pth and projection_head_supcon.pth to /content/drive/MyDrive/forensicbind/checkpoints\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 002 | train_loss = 5.4304 | val_loss = 4.6850\n",
            "  🔥 New best model at epoch 2, val_loss = 4.6850\n",
            "  Saved encoder_supcon.pth and projection_head_supcon.pth to /content/drive/MyDrive/forensicbind/checkpoints\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 003 | train_loss = 5.7285 | val_loss = 4.8690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 004 | train_loss = 5.5477 | val_loss = 4.6729\n",
            "  🔥 New best model at epoch 4, val_loss = 4.6729\n",
            "  Saved encoder_supcon.pth and projection_head_supcon.pth to /content/drive/MyDrive/forensicbind/checkpoints\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 005 | train_loss = 5.6226 | val_loss = 5.0339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 006 | train_loss = 5.3704 | val_loss = 4.8484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 007 | train_loss = 5.4122 | val_loss = 5.4246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 008 | train_loss = 5.2345 | val_loss = 5.0845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 009 | train_loss = 5.2055 | val_loss = 5.0411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 010 | train_loss = 5.3868 | val_loss = 4.7061\n",
            "\n",
            "SupCon training complete.\n",
            "Best val loss: 4.672924443826837 at epoch 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "supcon_config = {\n",
        "    \"model_name\": \"vit_base_patch16_224\",\n",
        "    \"img_size\": IMG_SIZE,\n",
        "    \"embedding_dim\": encoder.embed_dim,\n",
        "\n",
        "    # Projection head\n",
        "    \"proj_hidden_dim\": 2048,\n",
        "    \"proj_out_dim\": 128,\n",
        "\n",
        "    # SupCon hyperparameters\n",
        "    \"temperature\": supcon_temperature,\n",
        "    \"optimizer\": \"AdamW\",\n",
        "    \"learning_rate\": lr,\n",
        "    \"weight_decay\": weight_decay,\n",
        "    \"num_epochs\": num_epochs,\n",
        "    \"batch_size\": train_loader.batch_size,\n",
        "\n",
        "    # System info\n",
        "    \"use_amp\": use_amp,\n",
        "    \"device\": str(DEVICE),\n",
        "\n",
        "    # Downstream classification setup\n",
        "    \"num_classes\": NUM_CLASSES,  # ✅ should be 5 now\n",
        "}\n",
        "\n",
        "config_path = CHECKPOINT_DIR / \"supcon_config.json\"\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(supcon_config, f, indent=2)\n",
        "\n",
        "print(\"Saved SupCon config to:\", config_path)\n",
        "config_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Hej71VX3xX",
        "outputId": "a644ef87-6b92-4f89-aacd-8c61a74c99d8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved SupCon config to: /content/drive/MyDrive/forensicbind/checkpoints/supcon_config.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/forensicbind/checkpoints/supcon_config.json')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sMbhoRbxX-xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 4"
      ],
      "metadata": {
        "id": "OCdDYOMrX_ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# Device (should match earlier)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# ✅ Project + checkpoints (Drive-backed, consistent with earlier notebook)\n",
        "PROJECT_ROOT = DATA_ROOT\n",
        "CHECKPOINT_DIR = PROJECT_ROOT / \"checkpoints\"\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Checkpoint dir:\", CHECKPOINT_DIR.resolve())\n",
        "\n",
        "# -----------------------------\n",
        "# Re-create encoder (same arch as SupCon)\n",
        "# -----------------------------\n",
        "encoder = ViTEncoder(\n",
        "    model_name=\"vit_base_patch16_224\",\n",
        "    pretrained=False,        # we will load SupCon weights instead\n",
        ").to(DEVICE)\n",
        "\n",
        "encoder_ckpt_path = CHECKPOINT_DIR / \"encoder_supcon.pth\"\n",
        "print(\"Loading encoder weights from:\", encoder_ckpt_path)\n",
        "assert encoder_ckpt_path.exists(), f\"Missing encoder checkpoint: {encoder_ckpt_path}\"\n",
        "\n",
        "state_dict = torch.load(encoder_ckpt_path, map_location=DEVICE)\n",
        "encoder.load_state_dict(state_dict)\n",
        "\n",
        "# Freeze encoder for linear-probe stage\n",
        "encoder.freeze_all()\n",
        "encoder.eval()\n",
        "\n",
        "# Small sanity print\n",
        "total_params = sum(p.numel() for p in encoder.parameters())\n",
        "trainable_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"\\nEncoder loaded and frozen.\")\n",
        "print(f\"Total params:     {total_params:,}\")\n",
        "print(f\"Trainable params: {trainable_params:,}\")\n",
        "print(\"Encoder embedding dim:\", encoder.embed_dim)\n",
        "print(\"On device:\", next(encoder.parameters()).device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acUc0T0oYBEY",
        "outputId": "0718aa0a-1d45-4c71-a7c8-c9ce430ffdaa"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Checkpoint dir: /content/drive/MyDrive/forensicbind/checkpoints\n",
            "Loading encoder weights from: /content/drive/MyDrive/forensicbind/checkpoints/encoder_supcon.pth\n",
            "\n",
            "Encoder loaded and frozen.\n",
            "Total params:     85,798,656\n",
            "Trainable params: 0\n",
            "Encoder embedding dim: 768\n",
            "On device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-class setup (must match CLASS_NAMES defined earlier)\n",
        "NUM_CLASSES = len(CLASS_NAMES)   # should be 5: Real, Deepfakes, Face2Face, FaceSwap, NeuralTextures\n",
        "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
        "\n",
        "class AttributionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Linear-probe + (optional) fine-tuning model:\n",
        "      - encoder: ViTEncoder returning CLS embedding\n",
        "      - classifier: linear layer mapping embed_dim -> NUM_CLASSES\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, num_classes=None):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        if num_classes is None:\n",
        "            num_classes = NUM_CLASSES\n",
        "        self.classifier = nn.Linear(self.encoder.embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.encoder(x)            # [B, embed_dim]\n",
        "        logits = self.classifier(feats)    # [B, num_classes]\n",
        "        return logits\n",
        "\n",
        "\n",
        "model = AttributionModel(encoder).to(DEVICE)\n",
        "print(model)\n",
        "\n",
        "# Sanity check: print head shape\n",
        "print(\"\\nClassifier in_features:\", model.classifier.in_features)\n",
        "print(\"Classifier out_features:\", model.classifier.out_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7H7LWitYQqr",
        "outputId": "f301d984-a2ba-4be6-c902-b1bb1dff141d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NUM_CLASSES: 5\n",
            "AttributionModel(\n",
            "  (encoder): ViTEncoder(\n",
            "    (backbone): VisionTransformer(\n",
            "      (patch_embed): PatchEmbed(\n",
            "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "        (norm): Identity()\n",
            "      )\n",
            "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "      (patch_drop): Identity()\n",
            "      (norm_pre): Identity()\n",
            "      (blocks): Sequential(\n",
            "        (0): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (1): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (2): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (3): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (4): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (5): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (6): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (7): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (8): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (9): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (10): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "        (11): Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (q_norm): Identity()\n",
            "            (k_norm): Identity()\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls1): Identity()\n",
            "          (drop_path1): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.0, inplace=False)\n",
            "            (norm): Identity()\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop2): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ls2): Identity()\n",
            "          (drop_path2): Identity()\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (fc_norm): Identity()\n",
            "      (head_drop): Dropout(p=0.0, inplace=False)\n",
            "      (head): Identity()\n",
            "    )\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
            ")\n",
            "\n",
            "Classifier in_features: 768\n",
            "Classifier out_features: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "\n",
        "SPLITS_DIR = DATA_ROOT / \"splits\"\n",
        "train_csv = SPLITS_DIR / \"train.csv\"\n",
        "val_csv   = SPLITS_DIR / \"val.csv\"\n",
        "test_csv  = SPLITS_DIR / \"test.csv\"\n",
        "\n",
        "assert train_csv.exists(), f\"train.csv not found at {train_csv}\"\n",
        "assert val_csv.exists(),   f\"val.csv not found at {val_csv}\"\n",
        "assert test_csv.exists(),  f\"test.csv not found at {test_csv}\"\n",
        "\n",
        "eval_transform = get_eval_transform()\n",
        "\n",
        "# ---- Classifier datasets (single view) ----\n",
        "train_dataset_cls = SupConImageDataset(\n",
        "    csv_path=train_csv,\n",
        "    transform=eval_transform,\n",
        "    two_views=False,\n",
        ")\n",
        "val_dataset_cls = SupConImageDataset(\n",
        "    csv_path=val_csv,\n",
        "    transform=eval_transform,\n",
        "    two_views=False,\n",
        ")\n",
        "test_dataset_cls = SupConImageDataset(\n",
        "    csv_path=test_csv,\n",
        "    transform=eval_transform,\n",
        "    two_views=False,\n",
        ")\n",
        "\n",
        "# Some quick stats\n",
        "print(\"Train set size:\", len(train_dataset_cls))\n",
        "print(\"Val set size:  \", len(val_dataset_cls))\n",
        "print(\"Test set size: \", len(test_dataset_cls))\n",
        "\n",
        "print(\"Train label distribution:\", train_dataset_cls.df[\"class_label\"].value_counts().sort_index().to_dict())\n",
        "print(\"Val label distribution:  \", val_dataset_cls.df[\"class_label\"].value_counts().sort_index().to_dict())\n",
        "print(\"Test label distribution: \", test_dataset_cls.df[\"class_label\"].value_counts().sort_index().to_dict())\n",
        "\n",
        "batch_size = 64  # try 64; if OOM, drop to 32\n",
        "num_workers = 4  # reduce to 2 if Colab complains\n",
        "\n",
        "# ✅ Weighted sampling for classifier train loader (handles imbalance)\n",
        "labels = train_dataset_cls.df[\"class_label\"].values\n",
        "class_counts = train_dataset_cls.df[\"class_label\"].value_counts().sort_index()\n",
        "class_weights = 1.0 / class_counts\n",
        "sample_weights = class_weights[labels].values\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=torch.as_tensor(sample_weights, dtype=torch.double),\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True,\n",
        ")\n",
        "\n",
        "train_loader_cls = DataLoader(\n",
        "    train_dataset_cls,\n",
        "    batch_size=batch_size,\n",
        "    sampler=sampler,     # ✅ replaces shuffle=True\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "val_loader_cls = DataLoader(\n",
        "    val_dataset_cls,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "test_loader_cls = DataLoader(\n",
        "    test_dataset_cls,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# Quick sanity check\n",
        "x, y = next(iter(train_loader_cls))\n",
        "print(\"Train batch shapes:\", x.shape, y.shape)\n",
        "print(\"Unique labels in first batch:\", y.unique().tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y5SV8hXYZ_d",
        "outputId": "452eebe9-1c24-4773-e77c-e1c3a12aaac3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 17065\n",
            "Val set size:   3762\n",
            "Test set size:  3653\n",
            "Train label distribution: {0: 4985, 1: 2800, 2: 3600, 3: 3680, 4: 2000}\n",
            "Val label distribution:   {0: 1162, 1: 600, 2: 760, 3: 800, 4: 440}\n",
            "Test label distribution:  {0: 1053, 1: 600, 2: 800, 3: 800, 4: 400}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batch shapes: torch.Size([64, 3, 224, 224]) torch.Size([64])\n",
            "Unique labels in first batch: [0, 1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "BBMEU5UuYQro"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "def evaluate_classifier(model, dataloader, device, return_logits=False):\n",
        "    \"\"\"\n",
        "    Evaluates classifier-only model (linear probe or fine-tuned encoder).\n",
        "\n",
        "    Returns:\n",
        "        acc          : accuracy\n",
        "        f1_macro     : macro F1 (important for imbalanced/multi-class)\n",
        "        cm           : confusion matrix (NUM_CLASSES × NUM_CLASSES)\n",
        "        labels_all   : true labels\n",
        "        preds_all    : predicted labels\n",
        "        logits_all?  : optional (for detailed error analysis)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    all_logits = [] if return_logits else None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, labels in dataloader:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            logits = model(x)   # [B, NUM_CLASSES]\n",
        "\n",
        "            # Protect against NaNs (e.g., bad weights)\n",
        "            if torch.isnan(logits).any():\n",
        "                print(\"⚠️ Warning: NaNs detected in logits. Replacing with zeros.\")\n",
        "                logits = torch.nan_to_num(logits)\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            if return_logits:\n",
        "                all_logits.append(logits.cpu().numpy())\n",
        "\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    if return_logits:\n",
        "        all_logits = np.concatenate(all_logits)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "    # ✅ Force fixed size matrix even if some labels are missing in a split\n",
        "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(NUM_CLASSES)))\n",
        "\n",
        "    if return_logits:\n",
        "        return acc, f1_macro, cm, all_labels, all_preds, all_logits\n",
        "    else:\n",
        "        return acc, f1_macro, cm, all_labels, all_preds\n"
      ],
      "metadata": {
        "id": "rU0nX1S1YfD2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Compute class weights from train set\n",
        "# -----------------------------\n",
        "train_labels = train_dataset_cls.df[\"class_label\"].tolist()\n",
        "label_counts = Counter(train_labels)\n",
        "\n",
        "print(\"Train frame counts per class:\")\n",
        "for c in range(NUM_CLASSES):\n",
        "    print(f\"  {c} ({CLASS_NAMES[c]}): {label_counts.get(c, 0)}\")\n",
        "\n",
        "total = sum(label_counts.values())\n",
        "\n",
        "class_weights = []\n",
        "for c in range(NUM_CLASSES):\n",
        "    count_c = label_counts.get(c, 0)\n",
        "    # Avoid division by zero if a class is missing in the split\n",
        "    if count_c == 0:\n",
        "        w = 0.0\n",
        "    else:\n",
        "        # Inverse-frequency style weight: total / (num_classes * count_c)\n",
        "        w = total / (NUM_CLASSES * count_c)\n",
        "    class_weights.append(w)\n",
        "\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32, device=DEVICE)\n",
        "print(\"Class weights tensor:\", class_weights)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Optimizer for LINEAR PROBE ONLY\n",
        "# -----------------------------\n",
        "optimizer_cls = optim.AdamW(\n",
        "    model.classifier.parameters(),  # ONLY the head\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        ")\n",
        "\n",
        "num_epochs_cls = 30  # you can reduce to 15–20 if training is slow\n",
        "\n",
        "print(\"\\nLinear probe optimizer:\")\n",
        "print(\"  lr =\", 1e-3)\n",
        "print(\"  weight_decay =\", 1e-4)\n",
        "print(\"  num_epochs_cls =\", num_epochs_cls)\n",
        "\n",
        "# Optional AMP (speeds up forward pass even with frozen encoder)\n",
        "use_amp_cls = torch.cuda.is_available()\n",
        "scaler_cls = torch.cuda.amp.GradScaler(enabled=use_amp_cls)\n",
        "print(\"AMP for linear probe:\", use_amp_cls)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Linear probe training loop\n",
        "# -----------------------------\n",
        "best_val_f1 = 0.0\n",
        "best_epoch = -1\n",
        "\n",
        "train_history = {\"epoch\": [], \"train_loss\": [], \"val_acc\": [], \"val_f1\": []}\n",
        "\n",
        "print(\"\\nStarting linear probe training (encoder frozen)...\\n\")\n",
        "\n",
        "for epoch in range(1, num_epochs_cls + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    loop = tqdm(train_loader_cls, desc=f\"LP Epoch {epoch} [Train]\", leave=False)\n",
        "    for x, labels in loop:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        labels = labels.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        optimizer_cls.zero_grad(set_to_none=True)\n",
        "\n",
        "        if use_amp_cls:\n",
        "            with torch.cuda.amp.autocast(enabled=True):\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, labels)\n",
        "            scaler_cls.scale(loss).backward()\n",
        "            scaler_cls.step(optimizer_cls)\n",
        "            scaler_cls.update()\n",
        "        else:\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer_cls.step()\n",
        "\n",
        "        running_loss += float(loss.item())\n",
        "        n_batches += 1\n",
        "        loop.set_postfix(loss=float(loss.item()))\n",
        "\n",
        "    train_loss = running_loss / max(1, n_batches)\n",
        "\n",
        "    # ----------------- Validation -----------------\n",
        "    val_acc, val_f1, val_cm, _, _ = evaluate_classifier(model, val_loader_cls, DEVICE)\n",
        "\n",
        "    train_history[\"epoch\"].append(epoch)\n",
        "    train_history[\"train_loss\"].append(train_loss)\n",
        "    train_history[\"val_acc\"].append(val_acc)\n",
        "    train_history[\"val_f1\"].append(val_f1)\n",
        "\n",
        "    print(\n",
        "        f\"LP Epoch {epoch:03d} | \"\n",
        "        f\"train_loss={train_loss:.4f} | \"\n",
        "        f\"val_acc={val_acc:.4f} | \"\n",
        "        f\"val_f1={val_f1:.4f}\"\n",
        "    )\n",
        "\n",
        "    # ----------------- Save last -----------------\n",
        "    torch.save(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"encoder_state_dict\": model.encoder.state_dict(),\n",
        "            \"classifier_state_dict\": model.classifier.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer_cls.state_dict(),\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_acc\": val_acc,\n",
        "            \"val_f1\": val_f1,\n",
        "            \"class_weights\": class_weights.detach().cpu().numpy().tolist(),\n",
        "        },\n",
        "        CHECKPOINT_DIR / \"classifier_last.pth\",\n",
        "    )\n",
        "\n",
        "    # ----------------- Save best (by val F1) -----------------\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        best_epoch = epoch\n",
        "\n",
        "        # (A) linear head only\n",
        "        torch.save(model.classifier.state_dict(), CHECKPOINT_DIR / \"linear_probe.pth\")\n",
        "\n",
        "        # (B) full model (encoder + classifier) at best F1\n",
        "        torch.save(\n",
        "            {\n",
        "                \"encoder_state_dict\": model.encoder.state_dict(),\n",
        "                \"classifier_state_dict\": model.classifier.state_dict(),\n",
        "            },\n",
        "            CHECKPOINT_DIR / \"encoder_plus_classifier.pth\",\n",
        "        )\n",
        "\n",
        "        print(f\"  🔥 New best linear probe at epoch {epoch}, val_f1={val_f1:.4f}\")\n",
        "\n",
        "print(\"\\nLinear probe training complete.\")\n",
        "print(\"Best val F1:\", best_val_f1, \"at epoch\", best_epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ExGHamfYuqW",
        "outputId": "6d7f8014-e817-41a8-a3d1-4b342fc20b04"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train frame counts per class:\n",
            "  0 (Real): 4985\n",
            "  1 (Deepfakes): 2800\n",
            "  2 (Face2Face): 3600\n",
            "  3 (FaceSwap): 3680\n",
            "  4 (NeuralTextures): 2000\n",
            "Class weights tensor: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1612987937.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_cls = torch.cuda.amp.GradScaler(enabled=use_amp_cls)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6847, 1.2189, 0.9481, 0.9274, 1.7065], device='cuda:0')\n",
            "\n",
            "Linear probe optimizer:\n",
            "  lr = 0.001\n",
            "  weight_decay = 0.0001\n",
            "  num_epochs_cls = 30\n",
            "AMP for linear probe: True\n",
            "\n",
            "Starting linear probe training (encoder frozen)...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 1 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 001 | train_loss=1.6373 | val_acc=0.1111 | val_f1=0.0572\n",
            "  🔥 New best linear probe at epoch 1, val_f1=0.0572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 2 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 002 | train_loss=1.5628 | val_acc=0.1680 | val_f1=0.1549\n",
            "  🔥 New best linear probe at epoch 2, val_f1=0.1549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 3 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 003 | train_loss=1.5320 | val_acc=0.1302 | val_f1=0.1094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 4 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 004 | train_loss=1.5164 | val_acc=0.1422 | val_f1=0.1269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 5 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 005 | train_loss=1.5015 | val_acc=0.1536 | val_f1=0.1537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 6 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 006 | train_loss=1.4787 | val_acc=0.1826 | val_f1=0.1265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 7 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 007 | train_loss=1.4823 | val_acc=0.1366 | val_f1=0.1097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 8 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 008 | train_loss=1.4744 | val_acc=0.1691 | val_f1=0.1336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 9 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 009 | train_loss=1.4673 | val_acc=0.1446 | val_f1=0.1319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 10 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 010 | train_loss=1.4562 | val_acc=0.1337 | val_f1=0.1141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 11 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 011 | train_loss=1.4490 | val_acc=0.1765 | val_f1=0.1578\n",
            "  🔥 New best linear probe at epoch 11, val_f1=0.1578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 12 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 012 | train_loss=1.4458 | val_acc=0.1116 | val_f1=0.0708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 13 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 013 | train_loss=1.4388 | val_acc=0.1728 | val_f1=0.1325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 14 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 014 | train_loss=1.4509 | val_acc=0.1401 | val_f1=0.1301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 15 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 015 | train_loss=1.4323 | val_acc=0.1374 | val_f1=0.1280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 16 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 016 | train_loss=1.4219 | val_acc=0.1510 | val_f1=0.1431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 17 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 017 | train_loss=1.4333 | val_acc=0.1592 | val_f1=0.1411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 18 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 018 | train_loss=1.4065 | val_acc=0.1393 | val_f1=0.1261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 19 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 019 | train_loss=1.3920 | val_acc=0.1746 | val_f1=0.1578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 20 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 020 | train_loss=1.4175 | val_acc=0.1664 | val_f1=0.1371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 21 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 021 | train_loss=1.4221 | val_acc=0.1244 | val_f1=0.1009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 22 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 022 | train_loss=1.4088 | val_acc=0.1417 | val_f1=0.1270\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 23 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 023 | train_loss=1.3922 | val_acc=0.1419 | val_f1=0.1261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 24 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 024 | train_loss=1.3944 | val_acc=0.2044 | val_f1=0.1827\n",
            "  🔥 New best linear probe at epoch 24, val_f1=0.1827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 25 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 025 | train_loss=1.4308 | val_acc=0.1576 | val_f1=0.1527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 26 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 026 | train_loss=1.3881 | val_acc=0.1975 | val_f1=0.1618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 27 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 027 | train_loss=1.3819 | val_acc=0.1409 | val_f1=0.1383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 28 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 028 | train_loss=1.3937 | val_acc=0.1505 | val_f1=0.1368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 29 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 029 | train_loss=1.3930 | val_acc=0.1462 | val_f1=0.1323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rLP Epoch 30 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-1612987937.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LP Epoch 030 | train_loss=1.3593 | val_acc=0.1536 | val_f1=0.1394\n",
            "\n",
            "Linear probe training complete.\n",
            "Best val F1: 0.18268974977286284 at epoch 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload best linear-probe weights\n",
        "ckpt_path = CHECKPOINT_DIR / \"encoder_plus_classifier.pth\"\n",
        "print(\"Loading best linear-probe checkpoint from:\", ckpt_path)\n",
        "assert ckpt_path.exists(), f\"Missing checkpoint: {ckpt_path}\"\n",
        "\n",
        "best_ckpt = torch.load(ckpt_path, map_location=DEVICE)\n",
        "model.encoder.load_state_dict(best_ckpt[\"encoder_state_dict\"])\n",
        "model.classifier.load_state_dict(best_ckpt[\"classifier_state_dict\"])\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# Evaluate on test set\n",
        "test_acc, test_f1, test_cm, test_labels, test_preds = evaluate_classifier(\n",
        "    model, test_loader_cls, DEVICE\n",
        ")\n",
        "\n",
        "print(\"\\n==== Linear Probe – TEST performance ====\")\n",
        "print(\"TEST accuracy :\", f\"{test_acc:.4f}\")\n",
        "print(\"TEST macro F1:\", f\"{test_f1:.4f}\")\n",
        "print(\"Confusion matrix:\\n\", test_cm)\n",
        "\n",
        "# Optional: nice per-class report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"\\nClassification report (test):\")\n",
        "print(\n",
        "    classification_report(\n",
        "        test_labels,\n",
        "        test_preds,\n",
        "        labels=list(range(NUM_CLASSES)),\n",
        "        target_names=CLASS_NAMES,\n",
        "        digits=3,\n",
        "        zero_division=0,\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksyfPbGDY5XS",
        "outputId": "458cafe3-296d-44bf-a479-2be3605decde"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading best linear-probe checkpoint from: /content/drive/MyDrive/forensicbind/checkpoints/encoder_plus_classifier.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Linear Probe – TEST performance ====\n",
            "TEST accuracy : 0.1859\n",
            "TEST macro F1: 0.1685\n",
            "Confusion matrix:\n",
            " [[285 199 256 107 206]\n",
            " [135 123 169  48 125]\n",
            " [158 237 174  49 182]\n",
            " [279 142 123  74 182]\n",
            " [ 93 118 159   7  23]]\n",
            "\n",
            "Classification report (test):\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          Real      0.300     0.271     0.285      1053\n",
            "     Deepfakes      0.150     0.205     0.173       600\n",
            "     Face2Face      0.198     0.217     0.207       800\n",
            "      FaceSwap      0.260     0.092     0.136       800\n",
            "NeuralTextures      0.032     0.058     0.041       400\n",
            "\n",
            "      accuracy                          0.186      3653\n",
            "     macro avg      0.188     0.169     0.169      3653\n",
            "  weighted avg      0.215     0.186     0.190      3653\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Unfreeze last ViT blocks for fine-tuning\n",
        "# -----------------------------\n",
        "print(\"\\nUnfreezing last ViT blocks for fine-tuning...\")\n",
        "model.encoder.unfreeze_last_blocks(num_blocks=2)\n",
        "\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total params:     {total_params:,}\")\n",
        "print(f\"Trainable params: {trainable_params:,}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Optimizer for fine-tuning (encoder + head, 2 param groups)\n",
        "# -----------------------------\n",
        "enc_params = [p for p in model.encoder.parameters() if p.requires_grad]\n",
        "head_params = [p for p in model.classifier.parameters() if p.requires_grad]\n",
        "\n",
        "optimizer_ft = optim.AdamW(\n",
        "    [\n",
        "        {\"params\": enc_params, \"lr\": 1e-5},   # encoder LR (small)\n",
        "        {\"params\": head_params, \"lr\": 1e-4},  # head LR (bigger)\n",
        "    ],\n",
        "    weight_decay=1e-4,\n",
        ")\n",
        "\n",
        "ft_epochs = 5  # 5–10 is usually enough\n",
        "\n",
        "print(\"\\nStarting fine-tuning...\")\n",
        "print(\"  epochs:\", ft_epochs)\n",
        "print(\"  encoder lr:\", 1e-5)\n",
        "print(\"  head lr   :\", 1e-4)\n",
        "print(\"  wd        :\", 1e-4)\n",
        "\n",
        "# AMP\n",
        "use_amp_ft = torch.cuda.is_available()\n",
        "scaler_ft = torch.cuda.amp.GradScaler(enabled=use_amp_ft)\n",
        "print(\"AMP for fine-tuning:\", use_amp_ft)\n",
        "\n",
        "best_ft_val_f1 = 0.0\n",
        "best_ft_epoch = -1\n",
        "\n",
        "for epoch in range(1, ft_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    loop = tqdm(train_loader_cls, desc=f\"FT Epoch {epoch} [Train]\", leave=False)\n",
        "    for x, labels in loop:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        labels = labels.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        optimizer_ft.zero_grad(set_to_none=True)\n",
        "\n",
        "        if use_amp_ft:\n",
        "            with torch.cuda.amp.autocast(enabled=True):\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, labels)  # reuse class-weighted CE\n",
        "            scaler_ft.scale(loss).backward()\n",
        "            scaler_ft.step(optimizer_ft)\n",
        "            scaler_ft.update()\n",
        "        else:\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer_ft.step()\n",
        "\n",
        "        running_loss += float(loss.item())\n",
        "        n_batches += 1\n",
        "        loop.set_postfix(loss=float(loss.item()))\n",
        "\n",
        "    train_loss = running_loss / max(1, n_batches)\n",
        "\n",
        "    # Validate after each fine-tune epoch\n",
        "    val_acc, val_f1, val_cm, _, _ = evaluate_classifier(model, val_loader_cls, DEVICE)\n",
        "\n",
        "    print(\n",
        "        f\"FT Epoch {epoch:03d} | \"\n",
        "        f\"train_loss={train_loss:.4f} | \"\n",
        "        f\"val_acc={val_acc:.4f} | \"\n",
        "        f\"val_f1={val_f1:.4f}\"\n",
        "    )\n",
        "\n",
        "    # Save last fine-tune checkpoint\n",
        "    torch.save(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"encoder_state_dict\": model.encoder.state_dict(),\n",
        "            \"classifier_state_dict\": model.classifier.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer_ft.state_dict(),\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_acc\": val_acc,\n",
        "            \"val_f1\": val_f1,\n",
        "        },\n",
        "        CHECKPOINT_DIR / \"classifier_finetune_last.pth\",\n",
        "    )\n",
        "\n",
        "    # Save best fine-tuned model\n",
        "    if val_f1 > best_ft_val_f1:\n",
        "        best_ft_val_f1 = val_f1\n",
        "        best_ft_epoch = epoch\n",
        "\n",
        "        torch.save(\n",
        "            {\n",
        "                \"encoder_state_dict\": model.encoder.state_dict(),\n",
        "                \"classifier_state_dict\": model.classifier.state_dict(),\n",
        "            },\n",
        "            CHECKPOINT_DIR / \"encoder_plus_classifier_finetuned.pth\",\n",
        "        )\n",
        "        print(f\"  🔥 New best fine-tuned model at epoch {epoch}, val_f1={val_f1:.4f}\")\n",
        "\n",
        "print(\"\\nFine-tuning complete.\")\n",
        "print(\"Best FT val F1:\", best_ft_val_f1, \"at epoch\", best_ft_epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b32isOkZC66",
        "outputId": "d90f4567-fceb-471c-b344-14d337a6c0bc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4093743636.py:38: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_ft = torch.cuda.amp.GradScaler(enabled=use_amp_ft)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unfreezing last ViT blocks for fine-tuning...\n",
            "Total params:     85,802,501\n",
            "Trainable params: 14,211,845\n",
            "\n",
            "Starting fine-tuning...\n",
            "  epochs: 5\n",
            "  encoder lr: 1e-05\n",
            "  head lr   : 0.0001\n",
            "  wd        : 0.0001\n",
            "AMP for fine-tuning: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFT Epoch 1 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4093743636.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 001 | train_loss=1.3320 | val_acc=0.1571 | val_f1=0.1559\n",
            "  🔥 New best fine-tuned model at epoch 1, val_f1=0.1559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFT Epoch 2 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4093743636.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 002 | train_loss=1.3138 | val_acc=0.1746 | val_f1=0.1680\n",
            "  🔥 New best fine-tuned model at epoch 2, val_f1=0.1680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFT Epoch 3 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4093743636.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 003 | train_loss=1.2795 | val_acc=0.1653 | val_f1=0.1674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFT Epoch 4 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4093743636.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 004 | train_loss=1.2570 | val_acc=0.1632 | val_f1=0.1579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFT Epoch 5 [Train]:   0%|          | 0/267 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-4093743636.py:57: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 005 | train_loss=1.2365 | val_acc=0.1704 | val_f1=0.1675\n",
            "\n",
            "Fine-tuning complete.\n",
            "Best FT val F1: 0.1680319373845643 at epoch 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load class map\n",
        "with open(DATA_ROOT / \"class_map.json\", \"r\") as f:\n",
        "    class_map = json.load(f)\n",
        "\n",
        "# NUM_CLASSES should now be 5\n",
        "assert NUM_CLASSES == len(class_map), \\\n",
        "    f\"NUM_CLASSES ({NUM_CLASSES}) does not match class_map ({len(class_map)})\"\n",
        "\n",
        "# Build target names in correct index order\n",
        "target_names = [class_map[str(i)] for i in range(NUM_CLASSES)]\n",
        "\n",
        "print(\"\\n==== Classification Report (Test) ====\\n\")\n",
        "print(\n",
        "    classification_report(\n",
        "        test_labels,\n",
        "        test_preds,\n",
        "        labels=list(range(NUM_CLASSES)),   # force 5×5 reporting\n",
        "        target_names=target_names,\n",
        "        digits=3,\n",
        "        zero_division=0,\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qEKEn2YY6tC",
        "outputId": "ef1ceaee-867d-4ae3-e1ac-9849e79e83e4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Classification Report (Test) ====\n",
            "\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          Real      0.300     0.271     0.285      1053\n",
            "     Deepfakes      0.150     0.205     0.173       600\n",
            "     Face2Face      0.198     0.217     0.207       800\n",
            "      FaceSwap      0.260     0.092     0.136       800\n",
            "NeuralTextures      0.032     0.058     0.041       400\n",
            "\n",
            "      accuracy                          0.186      3653\n",
            "     macro avg      0.188     0.169     0.169      3653\n",
            "  weighted avg      0.215     0.186     0.190      3653\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "classifier_config = {\n",
        "    # Checkpoints\n",
        "    \"encoder_checkpoint\": str(encoder_ckpt_path),   # SupCon encoder\n",
        "    \"linear_probe_path\": str(CHECKPOINT_DIR / \"linear_probe.pth\"),\n",
        "    \"combined_model_path\": str(CHECKPOINT_DIR / \"encoder_plus_classifier.pth\"),\n",
        "\n",
        "    # Fine-tuned model (if available)\n",
        "    \"finetuned_model_path\": str(CHECKPOINT_DIR / \"encoder_plus_classifier_finetuned.pth\"),\n",
        "\n",
        "    # Model structure\n",
        "    \"num_classes\": NUM_CLASSES,        # ✅ should be 5 now\n",
        "    \"feature_dim\": encoder.embed_dim,  # CLS embedding dim (e.g., 768)\n",
        "\n",
        "    # Training hyperparameters (linear probe)\n",
        "    \"optimizer\": \"AdamW\",\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"num_epochs_linear_probe\": num_epochs_cls,\n",
        "\n",
        "    # Fine-tuning info\n",
        "    \"fine_tune_epochs\": 5,\n",
        "    \"fine_tune_learning_rate\": 1e-5,\n",
        "    \"fine_tune_unfrozen_blocks\": 2,\n",
        "\n",
        "    # Encoder settings\n",
        "    \"freeze_encoder_linear_probe\": True,   # frozen during linear probe\n",
        "    \"freeze_encoder_finetune\": False,      # last blocks unfrozen during fine-tuning\n",
        "}\n",
        "\n",
        "config_path = CHECKPOINT_DIR / \"classifier_config.json\"\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(classifier_config, f, indent=2)\n",
        "\n",
        "print(\"Saved classifier config to:\", config_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As8rS6oHZYnM",
        "outputId": "3e3fb3e0-40bc-4865-cd8d-5f5e42730be8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved classifier config to: /content/drive/MyDrive/forensicbind/checkpoints/classifier_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6H0B503YZkxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SAVES\n"
      ],
      "metadata": {
        "id": "FMK3mtvfZkzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# FINAL SAVE BLOCK — SAFE FOR 5-CLASS PROJECT\n",
        "# Saves ALL critical artifacts to Drive (DATA_ROOT)\n",
        "# ============================================\n",
        "\n",
        "import torch\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = DATA_ROOT\n",
        "CHECKPOINT_DIR = PROJECT_ROOT / \"checkpoints\"\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Saving to:\", CHECKPOINT_DIR.resolve())\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Save SupCon encoder weights\n",
        "# -----------------------------\n",
        "try:\n",
        "    torch.save(encoder.state_dict(), CHECKPOINT_DIR / \"encoder_supcon.pth\")\n",
        "    print(\"Saved: encoder_supcon.pth\")\n",
        "except Exception as e:\n",
        "    print(\"WARNING: encoder not found:\", e)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Save SupCon projection head\n",
        "# -----------------------------\n",
        "try:\n",
        "    torch.save(projection_head.state_dict(), CHECKPOINT_DIR / \"projection_head_supcon.pth\")\n",
        "    print(\"Saved: projection_head_supcon.pth\")\n",
        "except Exception as e:\n",
        "    print(\"WARNING: projection_head not found:\", e)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Save linear probe classifier head\n",
        "# -----------------------------\n",
        "try:\n",
        "    torch.save(model.classifier.state_dict(), CHECKPOINT_DIR / \"linear_probe.pth\")\n",
        "    print(\"Saved: linear_probe.pth\")\n",
        "except Exception as e:\n",
        "    print(\"WARNING: classifier not found:\", e)\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Save encoder + classifier combined\n",
        "# -----------------------------\n",
        "try:\n",
        "    torch.save(\n",
        "        {\n",
        "            \"encoder_state_dict\": model.encoder.state_dict(),\n",
        "            \"classifier_state_dict\": model.classifier.state_dict(),\n",
        "        },\n",
        "        CHECKPOINT_DIR / \"encoder_plus_classifier.pth\"\n",
        "    )\n",
        "    print(\"Saved: encoder_plus_classifier.pth\")\n",
        "except Exception as e:\n",
        "    print(\"WARNING: could not save encoder+classifier:\", e)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Save fine-tuned model (if exists)\n",
        "# -----------------------------\n",
        "ft_path = CHECKPOINT_DIR / \"encoder_plus_classifier_finetuned.pth\"\n",
        "if ft_path.exists():\n",
        "    print(\"Fine-tuned model detected — keeping:\", ft_path.name)\n",
        "else:\n",
        "    print(\"No fine-tuned model found (OK if you skipped FT).\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Save CORRECT 5-class map\n",
        "# -----------------------------\n",
        "class_map = {\n",
        "    \"0\": \"Real\",\n",
        "    \"1\": \"Deepfakes\",\n",
        "    \"2\": \"Face2Face\",\n",
        "    \"3\": \"FaceSwap\",\n",
        "    \"4\": \"NeuralTextures\"\n",
        "}\n",
        "\n",
        "with open(PROJECT_ROOT / \"class_map.json\", \"w\") as f:\n",
        "    json.dump(class_map, f, indent=2)\n",
        "print(\"Saved: class_map.json (5-class mapping)\")\n",
        "\n",
        "# -----------------------------\n",
        "# 7) Save SupCon config\n",
        "# -----------------------------\n",
        "supcon_config = {\n",
        "    \"model_name\": \"vit_base_patch16_224\",\n",
        "    \"embedding_dim\": encoder.embed_dim,\n",
        "    \"projection_dim\": 128,\n",
        "    \"hidden_dim\": 2048,\n",
        "    \"temperature\": supcon_temperature,\n",
        "    \"batch_size\": train_loader.batch_size,\n",
        "    \"num_classes\": NUM_CLASSES,\n",
        "}\n",
        "with open(CHECKPOINT_DIR / \"supcon_config.json\", \"w\") as f:\n",
        "    json.dump(supcon_config, f, indent=2)\n",
        "print(\"Saved: supcon_config.json\")\n",
        "\n",
        "# -----------------------------\n",
        "# 8) Save classifier config (5-class)\n",
        "# -----------------------------\n",
        "classifier_config = {\n",
        "    \"num_classes\": NUM_CLASSES,\n",
        "    \"embedding_dim\": encoder.embed_dim,\n",
        "    \"learning_rate_linear_probe\": 1e-3,\n",
        "    \"learning_rate_finetune\": 1e-5,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"freeze_encoder_linear_probe\": True,\n",
        "    \"fine_tune_last_blocks\": 2,\n",
        "}\n",
        "with open(CHECKPOINT_DIR / \"classifier_config.json\", \"w\") as f:\n",
        "    json.dump(classifier_config, f, indent=2)\n",
        "print(\"Saved: classifier_config.json\")\n",
        "\n",
        "# -----------------------------\n",
        "# 9) Final file listing\n",
        "# -----------------------------\n",
        "print(\"\\nFILES IN CHECKPOINT_DIR:\")\n",
        "for f in CHECKPOINT_DIR.glob(\"*\"):\n",
        "    print(\" •\", f.name)\n",
        "\n",
        "print(\"\\n✅ ALL IMPORTANT FILES SAVED — YOU CAN SLEEP SAFELY 😴\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkJWOVuMZl2Q",
        "outputId": "9d6f52ed-4e88-4d13-f47c-3dd51e6aaafc"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving to: /content/drive/MyDrive/forensicbind/checkpoints\n",
            "Saved: encoder_supcon.pth\n",
            "Saved: projection_head_supcon.pth\n",
            "Saved: linear_probe.pth\n",
            "Saved: encoder_plus_classifier.pth\n",
            "Fine-tuned model detected — keeping: encoder_plus_classifier_finetuned.pth\n",
            "Saved: class_map.json (5-class mapping)\n",
            "Saved: supcon_config.json\n",
            "Saved: classifier_config.json\n",
            "\n",
            "FILES IN CHECKPOINT_DIR:\n",
            " • supcon_last.pth\n",
            " • encoder_supcon.pth\n",
            " • projection_head_supcon.pth\n",
            " • supcon_config.json\n",
            " • classifier_last.pth\n",
            " • linear_probe.pth\n",
            " • encoder_plus_classifier.pth\n",
            " • classifier_finetune_last.pth\n",
            " • encoder_plus_classifier_finetuned.pth\n",
            " • classifier_config.json\n",
            "\n",
            "✅ ALL IMPORTANT FILES SAVED — YOU CAN SLEEP SAFELY 😴\n"
          ]
        }
      ]
    }
  ]
}
