{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f89ad715-911d-406f-9c9f-f4a0a407cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "\n",
    "import timm  # make sure timm is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0685a9e0-d5d1-44c1-92c4-069261b16190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT     : /home/ih2363/forensicbind/data_root\n",
      "CHECKPOINT_DIR: /home/ih2363/forensicbind/checkpoints\n",
      "Using device: cuda\n",
      "CLASS_NAMES: ['Real', 'FaceSwap', 'Face2Face']\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path(\"/home/ih2363/forensicbind/data_root\")\n",
    "PROJECT_ROOT = Path(\"/home/ih2363/forensicbind\")\n",
    "CHECKPOINT_DIR = PROJECT_ROOT / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATA_ROOT     :\", DATA_ROOT)\n",
    "print(\"CHECKPOINT_DIR:\", CHECKPOINT_DIR)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# We now ONLY care about 3 classes\n",
    "CLASS_NAMES = [\"Real\", \"FaceSwap\", \"Face2Face\"]\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "print(\"CLASS_NAMES:\", CLASS_NAMES)\n",
    "\n",
    "# Original labels mapping (for reference):\n",
    "#   0: Real\n",
    "#   1: FaceSwap\n",
    "#   2: Face2Face\n",
    "#   3: NeuralTextures  <-- WILL BE DROPPED\n",
    "\n",
    "# Hyperparams\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "NUM_EPOCHS = 20\n",
    "BASE_LR = 5e-5       # lower LR that worked better for you\n",
    "WEIGHT_DECAY = 3e-4  \n",
    "LABEL_SMOOTHING = 0.05   # start with 0; you can try 0.05 later\n",
    "VAL_EVERY = 1\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bb383ee-c88b-4174-b0ad-b451e1c6e57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits dir: /home/ih2363/forensicbind/data_root/splits\n",
      "  train.csv exists: True\n",
      "  val.csv   exists: True\n",
      "  test.csv  exists: True\n"
     ]
    }
   ],
   "source": [
    "SPLITS_DIR = DATA_ROOT / \"splits\"\n",
    "train_csv = SPLITS_DIR / \"train.csv\"\n",
    "val_csv   = SPLITS_DIR / \"val.csv\"\n",
    "test_csv  = SPLITS_DIR / \"test.csv\"\n",
    "\n",
    "print(\"Splits dir:\", SPLITS_DIR)\n",
    "print(\"  train.csv exists:\", train_csv.exists())\n",
    "print(\"  val.csv   exists:\", val_csv.exists())\n",
    "print(\"  test.csv  exists:\", test_csv.exists())\n",
    "\n",
    "assert train_csv.exists(), f\"Missing {train_csv}\"\n",
    "assert val_csv.exists(),   f\"Missing {val_csv}\"\n",
    "assert test_csv.exists(),  f\"Missing {test_csv}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11c67464-d4af-4c32-a66a-dc9e3b54f64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits dir: /home/ih2363/forensicbind/data_root/splits\n",
      "  train.csv exists: True\n",
      "  val.csv   exists: True\n",
      "  test.csv  exists: True\n",
      "[train.csv] dropped 420 NeuralTextures frames, kept 1599 samples.\n",
      "                                          frame_path  class_label\n",
      "0  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "1  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "2  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "3  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "4  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "[val.csv] dropped 120 NeuralTextures frames, kept 366 samples.\n",
      "                                          frame_path  class_label\n",
      "0  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "1  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "2  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "3  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "4  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "[test.csv] dropped 240 NeuralTextures frames, kept 264 samples.\n",
      "                                          frame_path  class_label\n",
      "0  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "1  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "2  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "3  /home/ih2363/forensicbind/data_root/original_s...            0\n",
      "4  /home/ih2363/forensicbind/data_root/original_s...            0\n"
     ]
    }
   ],
   "source": [
    "SPLITS_DIR = DATA_ROOT / \"splits\"\n",
    "train_csv = SPLITS_DIR / \"train.csv\"\n",
    "val_csv   = SPLITS_DIR / \"val.csv\"\n",
    "test_csv  = SPLITS_DIR / \"test.csv\"\n",
    "\n",
    "print(\"Splits dir:\", SPLITS_DIR)\n",
    "print(\"  train.csv exists:\", train_csv.exists())\n",
    "print(\"  val.csv   exists:\", val_csv.exists())\n",
    "print(\"  test.csv  exists:\", test_csv.exists())\n",
    "\n",
    "assert train_csv.exists(), f\"Missing {train_csv}\"\n",
    "assert val_csv.exists(),   f\"Missing {val_csv}\"\n",
    "assert test_csv.exists(),  f\"Missing {test_csv}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Cell 4: Dataset + transforms (drop NeuralTextures)\n",
    "# -----------------------------\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "def get_train_transform():\n",
    "    return T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.RandomResizedCrop(224, scale=(0.7, 1.0), ratio=(0.9, 1.1)),\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        T.RandomApply([T.GaussianBlur(kernel_size=3)], p=0.3),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "def get_eval_transform():\n",
    "    return T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "class DeepfakeFrameDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Frame-level dataset using your existing CSVs, but:\n",
    "      - drops any rows where class_label == 3 (NeuralTextures)\n",
    "      - keeps labels 0,1,2 as-is (Real, FaceSwap, Face2Face)\n",
    "    CSV must contain: frame_path, class_label\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path: Path, transform=None):\n",
    "        super().__init__()\n",
    "        self.csv_path = Path(csv_path)\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "\n",
    "        assert \"frame_path\"  in df.columns, \"CSV must contain 'frame_path'\"\n",
    "        assert \"class_label\" in df.columns, \"CSV must contain 'class_label'\"\n",
    "\n",
    "        df[\"class_label\"] = df[\"class_label\"].astype(int)\n",
    "\n",
    "        # DROP NeuralTextures (label == 3)\n",
    "        before = len(df)\n",
    "        df = df[df[\"class_label\"] != 3].reset_index(drop=True)\n",
    "        after = len(df)\n",
    "        print(f\"[{self.csv_path.name}] dropped {before - after} NeuralTextures frames, kept {after} samples.\")\n",
    "\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "        print(self.df[[\"frame_path\", \"class_label\"]].head())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _resolve_path(self, p: str) -> Path:\n",
    "        p = Path(p)\n",
    "        if p.is_absolute():\n",
    "            return p\n",
    "        return DATA_ROOT / p\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self._resolve_path(row[\"frame_path\"])\n",
    "        label = int(row[\"class_label\"])  # should be 0,1,2 only\n",
    "\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert(\"RGB\")\n",
    "        except Exception:\n",
    "            # if corrupted/missing, fall back to another index\n",
    "            new_idx = (idx + 1) % len(self.df)\n",
    "            row = self.df.iloc[new_idx]\n",
    "            img_path = self._resolve_path(row[\"frame_path\"])\n",
    "            label = int(row[\"class_label\"])\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# Instantiate datasets\n",
    "train_transform = get_train_transform()\n",
    "eval_transform  = get_eval_transform()\n",
    "\n",
    "train_dataset = DeepfakeFrameDataset(train_csv, transform=train_transform)\n",
    "val_dataset   = DeepfakeFrameDataset(val_csv,   transform=eval_transform)\n",
    "test_dataset  = DeepfakeFrameDataset(test_csv,  transform=eval_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5c6daab-a74b-4ee6-9ee3-1e19dc9ed8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train frame counts per class (3-class):\n",
      "  0 (Real): 566\n",
      "  1 (FaceSwap): 640\n",
      "  2 (Face2Face): 393\n",
      "Class distribution (proportions): [0.35397123 0.40025016 0.24577861]\n",
      "Dataloaders ready (3-class, no sampler).\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Cell 5: Class counts, loaders (NO sampler)\n",
    "# -----------------------------\n",
    "train_labels = train_dataset.df[\"class_label\"].values\n",
    "class_counts = np.bincount(train_labels, minlength=NUM_CLASSES).astype(float)\n",
    "print(\"Train frame counts per class (3-class):\")\n",
    "for idx, cnt in enumerate(class_counts):\n",
    "    print(f\"  {idx} ({CLASS_NAMES[idx]}): {int(cnt)}\")\n",
    "\n",
    "# Just to see distribution; we won't use it for sampler anymore\n",
    "print(\"Class distribution (proportions):\", class_counts / class_counts.sum())\n",
    "\n",
    "# Dataloaders WITHOUT sampler\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,         # <--- important now\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"Dataloaders ready (3-class, no sampler).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93f61770-ff6c-4ecb-a928-71cf8997e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNetDeepfake(\n",
      "  (backbone): EfficientNet(\n",
      "    (conv_stem): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNormAct2d(\n",
      "      32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "      (drop): Identity()\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (blocks): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): DepthwiseSeparableConv(\n",
      "          (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): InvertedResidual(\n",
      "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): InvertedResidual(\n",
      "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): InvertedResidual(\n",
      "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (3): InvertedResidual(\n",
      "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): SqueezeExcite(\n",
      "            (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (act1): SiLU(inplace=True)\n",
      "            (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (gate): Sigmoid()\n",
      "          )\n",
      "          (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn2): BatchNormAct2d(\n",
      "      1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "      (drop): Identity()\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
      "    (classifier): Identity()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (classifier): Linear(in_features=1280, out_features=3, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters:     4,011,391\n",
      "Trainable parameters: 4,011,391\n"
     ]
    }
   ],
   "source": [
    "BACKBONE_NAME = \"efficientnet_b0\"\n",
    "\n",
    "class EfficientNetDeepfake(nn.Module):\n",
    "    def __init__(self, num_classes: int = NUM_CLASSES, backbone_name: str = BACKBONE_NAME):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,   # feature extractor\n",
    "            in_chans=3,\n",
    "        )\n",
    "\n",
    "        if hasattr(self.backbone, \"num_features\"):\n",
    "            feat_dim = self.backbone.num_features\n",
    "        elif hasattr(self.backbone, \"classifier\") and hasattr(self.backbone.classifier, \"in_features\"):\n",
    "            feat_dim = self.backbone.classifier.in_features\n",
    "        else:\n",
    "            feat_dim = 1280  # fallback\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.classifier = nn.Linear(feat_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        feats = self.dropout(feats)\n",
    "        logits = self.classifier(feats)\n",
    "        return logits\n",
    "\n",
    "model = EfficientNetDeepfake(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters:     {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0715b6b2-4568-4941-aeda-83740e8fc742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5444/418415918.py:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Cell 7: Loss, optimizer, scheduler (no class weights)\n",
    "# -----------------------------\n",
    "if LABEL_SMOOTHING > 0.0:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=BASE_LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=NUM_EPOCHS,\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53ee1cdf-01ca-4278-8000-aa1ac555c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Cell 8: Epoch loop helper\n",
    "# -----------------------------\n",
    "def run_one_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer=None,\n",
    "    scaler=None,\n",
    "    device=DEVICE,\n",
    "    train: bool = True,\n",
    "):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n",
    "                logits = model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            with torch.no_grad(), torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n",
    "                logits = model(images)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "        epoch_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "        all_preds.append(preds.detach().cpu().numpy())\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "\n",
    "    avg_loss = epoch_loss / len(loader.dataset)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    return avg_loss, acc, macro_f1, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8716eb0d-46f9-450e-a633-b6da3a348159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training (3 classes: Real, FaceSwap, Face2Face)...\n",
      "  epochs   : 20\n",
      "  base lr  : 5e-05\n",
      "  batch_sz : 32\n",
      "Epoch 001 | train_loss=0.8985 | train_acc=0.7036 | train_f1=0.6926 || val_loss=1.0801 | val_acc=0.4016 | val_f1=0.3286 | time=6.3s\n",
      "  ðŸ”¥ New best model at epoch 1, val_f1=0.3286\n",
      "    Saved to: /home/ih2363/forensicbind/checkpoints/efficientnet_3class_best.pth\n",
      "Epoch 002 | train_loss=0.4700 | train_acc=0.9681 | train_f1=0.9675 || val_loss=0.9955 | val_acc=0.5328 | val_f1=0.4777 | time=6.8s\n",
      "  ðŸ”¥ New best model at epoch 2, val_f1=0.4777\n",
      "    Saved to: /home/ih2363/forensicbind/checkpoints/efficientnet_3class_best.pth\n",
      "Epoch 003 | train_loss=0.2633 | train_acc=0.9881 | train_f1=0.9871 || val_loss=0.9891 | val_acc=0.5874 | val_f1=0.5641 | time=6.3s\n",
      "  ðŸ”¥ New best model at epoch 3, val_f1=0.5641\n",
      "    Saved to: /home/ih2363/forensicbind/checkpoints/efficientnet_3class_best.pth\n",
      "Epoch 004 | train_loss=0.2129 | train_acc=0.9937 | train_f1=0.9934 || val_loss=0.9744 | val_acc=0.6148 | val_f1=0.6052 | time=6.3s\n",
      "  ðŸ”¥ New best model at epoch 4, val_f1=0.6052\n",
      "    Saved to: /home/ih2363/forensicbind/checkpoints/efficientnet_3class_best.pth\n",
      "Epoch 005 | train_loss=0.1920 | train_acc=0.9969 | train_f1=0.9964 || val_loss=0.9333 | val_acc=0.6011 | val_f1=0.5871 | time=6.3s\n",
      "Epoch 006 | train_loss=0.1905 | train_acc=0.9975 | train_f1=0.9973 || val_loss=1.0318 | val_acc=0.5710 | val_f1=0.5541 | time=6.2s\n",
      "Epoch 007 | train_loss=0.1894 | train_acc=0.9987 | train_f1=0.9986 || val_loss=0.9684 | val_acc=0.5820 | val_f1=0.5672 | time=6.3s\n",
      "Epoch 008 | train_loss=0.1881 | train_acc=0.9987 | train_f1=0.9986 || val_loss=0.9517 | val_acc=0.5820 | val_f1=0.5665 | time=6.5s\n",
      "Epoch 009 | train_loss=0.1895 | train_acc=0.9975 | train_f1=0.9972 || val_loss=1.0081 | val_acc=0.5683 | val_f1=0.5539 | time=6.3s\n",
      "Epoch 010 | train_loss=0.1880 | train_acc=0.9987 | train_f1=0.9986 || val_loss=0.9788 | val_acc=0.5710 | val_f1=0.5556 | time=6.4s\n",
      "Epoch 011 | train_loss=0.1836 | train_acc=1.0000 | train_f1=1.0000 || val_loss=0.9799 | val_acc=0.5820 | val_f1=0.5679 | time=6.3s\n",
      "Epoch 012 | train_loss=0.1835 | train_acc=0.9981 | train_f1=0.9980 || val_loss=0.9972 | val_acc=0.5820 | val_f1=0.5684 | time=6.4s\n",
      "Epoch 013 | train_loss=0.1829 | train_acc=0.9994 | train_f1=0.9994 || val_loss=0.9758 | val_acc=0.5902 | val_f1=0.5782 | time=6.4s\n",
      "Epoch 014 | train_loss=0.1796 | train_acc=1.0000 | train_f1=1.0000 || val_loss=0.9750 | val_acc=0.5984 | val_f1=0.5893 | time=6.2s\n",
      "Epoch 015 | train_loss=0.1828 | train_acc=1.0000 | train_f1=1.0000 || val_loss=1.0408 | val_acc=0.5683 | val_f1=0.5523 | time=6.3s\n",
      "Epoch 016 | train_loss=0.1849 | train_acc=0.9994 | train_f1=0.9994 || val_loss=0.9616 | val_acc=0.6120 | val_f1=0.6015 | time=6.3s\n",
      "Epoch 017 | train_loss=0.1820 | train_acc=1.0000 | train_f1=1.0000 || val_loss=0.9859 | val_acc=0.5902 | val_f1=0.5742 | time=6.3s\n",
      "Epoch 018 | train_loss=0.1816 | train_acc=1.0000 | train_f1=1.0000 || val_loss=0.9686 | val_acc=0.5929 | val_f1=0.5799 | time=6.4s\n",
      "Epoch 019 | train_loss=0.1806 | train_acc=0.9994 | train_f1=0.9994 || val_loss=0.9814 | val_acc=0.5956 | val_f1=0.5859 | time=6.5s\n",
      "Epoch 020 | train_loss=0.1802 | train_acc=1.0000 | train_f1=1.0000 || val_loss=0.9646 | val_acc=0.6011 | val_f1=0.5871 | time=6.3s\n",
      "\n",
      "Training complete (3-class).\n",
      "Best val F1: 0.6052 at epoch 4\n",
      "Best checkpoint: /home/ih2363/forensicbind/checkpoints/efficientnet_3class_best.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Cell 9: Training loop (3-class)\n",
    "# -----------------------------\n",
    "best_val_f1 = 0.0\n",
    "best_epoch = -1\n",
    "best_ckpt_path = CHECKPOINT_DIR / \"efficientnet_3class_best.pth\"\n",
    "\n",
    "print(\"\\nStarting training (3 classes: Real, FaceSwap, Face2Face)...\")\n",
    "print(f\"  epochs   : {NUM_EPOCHS}\")\n",
    "print(f\"  base lr  : {BASE_LR}\")\n",
    "print(f\"  batch_sz : {BATCH_SIZE}\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc, train_f1, _, _ = run_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer=optimizer,\n",
    "        scaler=scaler,\n",
    "        device=DEVICE,\n",
    "        train=True,\n",
    "    )\n",
    "\n",
    "    if (epoch % VAL_EVERY) == 0:\n",
    "        val_loss, val_acc, val_f1, _, _ = run_one_epoch(\n",
    "            model,\n",
    "            val_loader,\n",
    "            optimizer=None,\n",
    "            scaler=scaler,\n",
    "            device=DEVICE,\n",
    "            train=False,\n",
    "        )\n",
    "    else:\n",
    "        val_loss = val_acc = val_f1 = float(\"nan\")\n",
    "\n",
    "    scheduler.step()\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d} | \"\n",
    "        f\"train_loss={train_loss:.4f} | train_acc={train_acc:.4f} | train_f1={train_f1:.4f} || \"\n",
    "        f\"val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | val_f1={val_f1:.4f} | \"\n",
    "        f\"time={elapsed:.1f}s\"\n",
    "    )\n",
    "\n",
    "    if not math.isnan(val_f1) and val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_epoch = epoch\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"scheduler_state\": scheduler.state_dict(),\n",
    "                \"val_f1\": best_val_f1,\n",
    "                \"class_names\": CLASS_NAMES,\n",
    "            },\n",
    "            best_ckpt_path,\n",
    "        )\n",
    "        print(f\"  ðŸ”¥ New best model at epoch {epoch}, val_f1={best_val_f1:.4f}\")\n",
    "        print(f\"    Saved to: {best_ckpt_path}\")\n",
    "\n",
    "print(\"\\nTraining complete (3-class).\")\n",
    "print(f\"Best val F1: {best_val_f1:.4f} at epoch {best_epoch}\")\n",
    "print(f\"Best checkpoint: {best_ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a284913-c39a-47d3-a566-a59913b3b08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading best checkpoint for TEST evaluation (3-class)...\n",
      "\n",
      "==== EfficientNet â€“ TEST performance (3-class) ====\n",
      "TEST loss    : 0.7983\n",
      "TEST accuracy: 0.6288\n",
      "TEST macro F1: 0.6007\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[41 32 47]\n",
      " [ 6 92  0]\n",
      " [ 7  6 33]]\n",
      "\n",
      "Classification report (test, 3-class):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real      0.759     0.342     0.471       120\n",
      "    FaceSwap      0.708     0.939     0.807        98\n",
      "   Face2Face      0.412     0.717     0.524        46\n",
      "\n",
      "    accuracy                          0.629       264\n",
      "   macro avg      0.626     0.666     0.601       264\n",
      "weighted avg      0.680     0.629     0.605       264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Cell 10: Load best and TEST evaluation (3-class)\n",
    "# -----------------------------\n",
    "print(\"\\nLoading best checkpoint for TEST evaluation (3-class)...\")\n",
    "ckpt = torch.load(best_ckpt_path, map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "test_loss, test_acc, test_f1, test_labels, test_preds = run_one_epoch(\n",
    "    model,\n",
    "    test_loader,\n",
    "    optimizer=None,\n",
    "    scaler=scaler,\n",
    "    device=DEVICE,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "print(\"\\n==== EfficientNet â€“ TEST performance (3-class) ====\")\n",
    "print(f\"TEST loss    : {test_loss:.4f}\")\n",
    "print(f\"TEST accuracy: {test_acc:.4f}\")\n",
    "print(f\"TEST macro F1: {test_f1:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_preds, labels=list(range(NUM_CLASSES)))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification report (test, 3-class):\")\n",
    "print(classification_report(test_labels, test_preds, target_names=CLASS_NAMES, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1b0e84b-39a6-4055-a6e3-7c1574539770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export dir: /home/ih2363/forensicbind/export\n",
      "Saved model weights to: /home/ih2363/forensicbind/export/efficientnet_3class.pth\n",
      "Saved model config to: /home/ih2363/forensicbind/export/model_config.json\n",
      "Saved test metrics to: /home/ih2363/forensicbind/export/test_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FINAL CELL â€” SAVE WEIGHTS, CONFIG, METRICS\n",
    "# ============================================================\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Export directory\n",
    "EXPORT_DIR = PROJECT_ROOT / \"export\"\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Export dir:\", EXPORT_DIR)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Save the model weights (state_dict only)\n",
    "# ------------------------------------------------------------\n",
    "weights_path = EXPORT_DIR / \"efficientnet_3class.pth\"\n",
    "torch.save(model.state_dict(), weights_path)\n",
    "print(\"Saved model weights to:\", weights_path)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Save model configuration (for anyone loading the model)\n",
    "# ------------------------------------------------------------\n",
    "model_config = {\n",
    "    \"backbone_name\": BACKBONE_NAME,        # e.g. \"efficientnet_b0\"\n",
    "    \"num_classes\": int(NUM_CLASSES),       # should be 3\n",
    "    \"class_names\": CLASS_NAMES,            # [\"Real\", \"FaceSwap\", \"Face2Face\"]\n",
    "    \"input_size\": [3, 224, 224],           # channels, height, width\n",
    "    \"imagenet_mean\": IMAGENET_MEAN,\n",
    "    \"imagenet_std\": IMAGENET_STD,\n",
    "    \"dropout\": 0.3,\n",
    "    \"checkpoint\": \"efficientnet_3class.pth\"\n",
    "}\n",
    "\n",
    "config_path = EXPORT_DIR / \"model_config.json\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "print(\"Saved model config to:\", config_path)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Save test metrics (if test was already run)\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    test_metrics = {\n",
    "        \"test_loss\": float(test_loss),\n",
    "        \"test_accuracy\": float(test_acc),\n",
    "        \"test_macro_f1\": float(test_f1),\n",
    "    }\n",
    "    metrics_path = EXPORT_DIR / \"test_metrics.json\"\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(test_metrics, f, indent=2)\n",
    "    print(\"Saved test metrics to:\", metrics_path)\n",
    "\n",
    "except NameError:\n",
    "    print(\"âš ï¸  test_loss/test_acc/test_f1 not defined â€” \"\n",
    "          \"run the test evaluation cell first if you want metrics exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf3b0c-54b8-459a-b3ae-82b06e0d826a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52b6dc-58d9-40a9-90df-f22f3374998f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
