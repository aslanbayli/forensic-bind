{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deepfake Detection Analysis & XAI\n",
        "\n",
        "This notebook consolidates model loading, inference, and Explainable AI (XAI) analysis.\n",
        "It implements **GradCAM** and **LIME** to visualize which parts of the image contribute to the deepfake detection decision.\n",
        "\n",
        "**Author:** Team Member 3 (Analysis & Detection)\n",
        "\n",
        "## Contents\n",
        "1. Setup & Dependencies\n",
        "2. Model Architecture\n",
        "3. Preprocessing Pipeline\n",
        "4. XAI Methods (GradCAM, LIME)\n",
        "5. Visualization Utilities\n",
        "6. Single Image Analysis\n",
        "7. Batch Analysis & Comparison\n",
        "8. Results Export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages \n",
        "# %pip install torch torchvision timm opencv-python matplotlib pillow lime numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/aslanbayli/Documents/nyu/advanced-cv/forensic-bind/src\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup & Dependencies\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "import timm\n",
        "from abc import ABC, abstractmethod\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check for CUDA\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Project paths\n",
        "PROJECT_ROOT = Path(\"../\").resolve()\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration (from v1/efficientnet/model_config.json)\n",
        "CONFIG = {\n",
        "    \"num_classes\": 3,\n",
        "    \"backbone_name\": \"efficientnet_b0\",\n",
        "    \"imagenet_mean\": [0.485, 0.456, 0.406],\n",
        "    \"imagenet_std\": [0.229, 0.224, 0.225],\n",
        "    \"class_names\": [\"Real\", \"FaceSwap\", \"Face2Face\"],\n",
        "    \"input_size\": (224, 224),\n",
        "}\n",
        "\n",
        "NUM_CLASSES = CONFIG[\"num_classes\"]\n",
        "BACKBONE_NAME = CONFIG[\"backbone_name\"]\n",
        "IMAGENET_MEAN = CONFIG[\"imagenet_mean\"]\n",
        "IMAGENET_STD = CONFIG[\"imagenet_std\"]\n",
        "CLASS_NAMES = CONFIG[\"class_names\"]\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "print(f\"  Classes: {CLASS_NAMES}\")\n",
        "print(f\"  Backbone: {BACKBONE_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Architecture\n",
        "\n",
        "EfficientNet-based deepfake classifier using the `timm` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientNetDeepfake(nn.Module):\n",
        "    \"\"\"\n",
        "    EfficientNet-based deepfake classifier.\n",
        "    Uses timm library for the backbone.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int = NUM_CLASSES, backbone_name: str = BACKBONE_NAME):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            backbone_name,\n",
        "            pretrained=False,\n",
        "            num_classes=0,  # Remove classification head\n",
        "            in_chans=3,\n",
        "        )\n",
        "        # Get feature dimension dynamically\n",
        "        if hasattr(self.backbone, \"num_features\"):\n",
        "            feat_dim = self.backbone.num_features\n",
        "        elif hasattr(self.backbone, \"classifier\") and hasattr(self.backbone.classifier, \"in_features\"):\n",
        "            feat_dim = self.backbone.classifier.in_features\n",
        "        else:\n",
        "            feat_dim = 1280  # Default for efficientnet_b0\n",
        "        \n",
        "        self.classifier = nn.Linear(feat_dim, num_classes)\n",
        "        self.feat_dim = feat_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)\n",
        "        logits = self.classifier(feats)\n",
        "        return logits\n",
        "    \n",
        "    def get_features(self, x):\n",
        "        \"\"\"Get feature embeddings without classification.\"\"\"\n",
        "        return self.backbone(x)\n",
        "\n",
        "print(\"EfficientNetDeepfake model class defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_eval_transform():\n",
        "    \"\"\"Standard evaluation transform matching training.\"\"\"\n",
        "    return T.Compose([\n",
        "        T.Resize((256, 256)),\n",
        "        T.CenterCrop(224),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "    ])\n",
        "\n",
        "def load_image(path: str) -> Image.Image:\n",
        "    \"\"\"Load an image from path.\"\"\"\n",
        "    return Image.open(path).convert(\"RGB\")\n",
        "\n",
        "def preprocess_image(image: Image.Image) -> torch.Tensor:\n",
        "    \"\"\"Preprocess a PIL image for model input.\"\"\"\n",
        "    transform = get_eval_transform()\n",
        "    return transform(image).unsqueeze(0)\n",
        "\n",
        "def denormalize(tensor: torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"Convert normalized tensor back to displayable image.\"\"\"\n",
        "    mean = torch.tensor(IMAGENET_MEAN).view(3, 1, 1)\n",
        "    std = torch.tensor(IMAGENET_STD).view(3, 1, 1)\n",
        "    tensor = tensor.cpu().clone()\n",
        "    tensor = tensor * std + mean\n",
        "    tensor = torch.clamp(tensor, 0, 1)\n",
        "    return (tensor.squeeze().permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
        "\n",
        "print(\"Preprocessing functions defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. XAI Methods\n",
        "\n",
        "### Base Explainer Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseExplainer(ABC):\n",
        "    \"\"\"\n",
        "    Abstract base class for all explanation methods.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: nn.Module, device: str = 'cpu'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    @abstractmethod\n",
        "    def explain(self, input_tensor: torch.Tensor, target_class: int = None) -> np.ndarray:\n",
        "        \"\"\"Generate explanation heatmap.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def predict(self, input_tensor: torch.Tensor) -> Tuple[int, np.ndarray]:\n",
        "        \"\"\"Get prediction and probabilities.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(input_tensor.to(self.device))\n",
        "            probs = F.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "            pred_class = int(probs.argmax())\n",
        "        return pred_class, probs\n",
        "\n",
        "print(\"BaseExplainer class defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GradCAM Explainer\n",
        "\n",
        "Gradient-weighted Class Activation Mapping highlights regions that contribute most to the prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GradCAMExplainer(BaseExplainer):\n",
        "    \"\"\"\n",
        "    Gradient-weighted Class Activation Mapping (GradCAM).\n",
        "    Highlights regions that contribute most to the prediction.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: nn.Module, target_layer: nn.Module, device: str = 'cpu'):\n",
        "        super().__init__(model, device)\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        \n",
        "        # Register hooks\n",
        "        self.target_layer.register_forward_hook(self._save_activation)\n",
        "        self.target_layer.register_full_backward_hook(self._save_gradient)\n",
        "\n",
        "    def _save_activation(self, module, input, output):\n",
        "        self.activations = output.detach()\n",
        "\n",
        "    def _save_gradient(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0].detach()\n",
        "\n",
        "    def explain(self, input_tensor: torch.Tensor, target_class: int = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute GradCAM heatmap.\n",
        "        \n",
        "        Args:\n",
        "            input_tensor: Preprocessed image tensor (1, C, H, W)\n",
        "            target_class: Class index to explain (default: predicted class)\n",
        "            \n",
        "        Returns:\n",
        "            Normalized heatmap (H, W) in range [0, 1]\n",
        "        \"\"\"\n",
        "        input_tensor = input_tensor.to(self.device)\n",
        "        input_tensor.requires_grad = True\n",
        "        \n",
        "        # Forward pass\n",
        "        output = self.model(input_tensor)\n",
        "        if target_class is None:\n",
        "            target_class = output.argmax(dim=1).item()\n",
        "        \n",
        "        # Backward pass\n",
        "        self.model.zero_grad()\n",
        "        score = output[0, target_class]\n",
        "        score.backward()\n",
        "        \n",
        "        # Compute GradCAM\n",
        "        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)\n",
        "        cam = torch.sum(weights * self.activations, dim=1, keepdim=True)\n",
        "        cam = F.relu(cam)\n",
        "        \n",
        "        # Resize to input size\n",
        "        cam = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "        cam = cam.squeeze().cpu().numpy()\n",
        "        \n",
        "        # Normalize\n",
        "        cam = cam - cam.min()\n",
        "        cam = cam / (cam.max() + 1e-8)\n",
        "        \n",
        "        return cam\n",
        "\n",
        "print(\"GradCAMExplainer class defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LIME Explainer\n",
        "\n",
        "Local Interpretable Model-agnostic Explanations - explains predictions by learning local surrogate models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LIMEExplainer(BaseExplainer):\n",
        "    \"\"\"\n",
        "    Local Interpretable Model-agnostic Explanations (LIME).\n",
        "    Explains predictions by learning local surrogate models.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: nn.Module, device: str = 'cpu', num_samples: int = 1000):\n",
        "        super().__init__(model, device)\n",
        "        self.num_samples = num_samples\n",
        "        self.explainer = None\n",
        "        \n",
        "        try:\n",
        "            from lime import lime_image\n",
        "            self.explainer = lime_image.LimeImageExplainer()\n",
        "            print(\"LIME initialized successfully.\")\n",
        "        except ImportError:\n",
        "            print(\"Warning: LIME not installed. Run `pip install lime`.\")\n",
        "    \n",
        "    def _batch_predict(self, images: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Prediction function for LIME.\"\"\"\n",
        "        # images: (N, H, W, C) numpy array in [0, 255]\n",
        "        transform = get_eval_transform()\n",
        "        batch = []\n",
        "        for img in images:\n",
        "            pil_img = Image.fromarray(img.astype(np.uint8))\n",
        "            batch.append(transform(pil_img))\n",
        "        \n",
        "        batch_tensor = torch.stack(batch).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = self.model(batch_tensor)\n",
        "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
        "        \n",
        "        return probs\n",
        "\n",
        "    def explain(self, input_tensor: torch.Tensor, target_class: int = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute LIME explanation.\n",
        "        \n",
        "        Args:\n",
        "            input_tensor: Preprocessed image tensor (1, C, H, W)\n",
        "            target_class: Class index to explain (default: predicted class)\n",
        "            \n",
        "        Returns:\n",
        "            Heatmap (H, W) showing feature importance\n",
        "        \"\"\"\n",
        "        if self.explainer is None:\n",
        "            print(\"LIME not available. Returning empty heatmap.\")\n",
        "            return np.zeros((224, 224))\n",
        "        \n",
        "        # Convert tensor to numpy image\n",
        "        image_np = denormalize(input_tensor)\n",
        "        \n",
        "        # Get prediction if target_class not specified\n",
        "        if target_class is None:\n",
        "            target_class, _ = self.predict(input_tensor)\n",
        "        \n",
        "        # Run LIME\n",
        "        explanation = self.explainer.explain_instance(\n",
        "            image_np,\n",
        "            self._batch_predict,\n",
        "            top_labels=NUM_CLASSES,\n",
        "            hide_color=0,\n",
        "            num_samples=self.num_samples,\n",
        "        )\n",
        "        \n",
        "        # Get heatmap for target class\n",
        "        _, mask = explanation.get_image_and_mask(\n",
        "            target_class,\n",
        "            positive_only=False,\n",
        "            num_features=10,\n",
        "            hide_rest=False,\n",
        "        )\n",
        "        \n",
        "        # Normalize mask\n",
        "        mask = mask.astype(np.float32)\n",
        "        mask = (mask - mask.min()) / (mask.max() - mask.min() + 1e-8)\n",
        "        \n",
        "        return mask\n",
        "\n",
        "print(\"LIMEExplainer class defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualization Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_heatmap(image: np.ndarray, heatmap: np.ndarray, \n",
        "                  alpha: float = 0.5, colormap: int = cv2.COLORMAP_JET) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Overlay heatmap on image.\n",
        "    \n",
        "    Args:\n",
        "        image: Original image (H, W, 3) in [0, 255]\n",
        "        heatmap: Heatmap (H, W) in [0, 1]\n",
        "        alpha: Opacity of heatmap overlay\n",
        "        colormap: OpenCV colormap\n",
        "        \n",
        "    Returns:\n",
        "        Blended image\n",
        "    \"\"\"\n",
        "    if heatmap.shape[:2] != image.shape[:2]:\n",
        "        heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
        "    \n",
        "    heatmap_uint8 = (heatmap * 255).astype(np.uint8)\n",
        "    heatmap_colored = cv2.applyColorMap(heatmap_uint8, colormap)\n",
        "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    overlay = cv2.addWeighted(image, 1 - alpha, heatmap_colored, alpha, 0)\n",
        "    return overlay\n",
        "\n",
        "def plot_single_explanation(image: np.ndarray, heatmap: np.ndarray, \n",
        "                            title: str = \"Explanation\", pred_info: str = None):\n",
        "    \"\"\"Plot original image, heatmap, and overlay side by side.\"\"\"\n",
        "    overlay = apply_heatmap(image, heatmap)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    \n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original\")\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    im = axes[1].imshow(heatmap, cmap='jet')\n",
        "    axes[1].set_title(\"Heatmap\")\n",
        "    axes[1].axis('off')\n",
        "    plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "    \n",
        "    axes[2].imshow(overlay)\n",
        "    axes[2].set_title(\"Overlay\")\n",
        "    axes[2].axis('off')\n",
        "    \n",
        "    if pred_info:\n",
        "        fig.suptitle(f\"{title}\\n{pred_info}\", fontsize=12)\n",
        "    else:\n",
        "        fig.suptitle(title, fontsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_method_comparison(image: np.ndarray, heatmaps: Dict[str, np.ndarray], \n",
        "                           pred_info: str = None, save_path: str = None):\n",
        "    \"\"\"\n",
        "    Compare multiple XAI methods side by side.\n",
        "    \n",
        "    Args:\n",
        "        image: Original image\n",
        "        heatmaps: Dict mapping method name to heatmap\n",
        "        pred_info: Prediction info string\n",
        "        save_path: Optional path to save the figure\n",
        "    \"\"\"\n",
        "    n_methods = len(heatmaps)\n",
        "    fig, axes = plt.subplots(2, n_methods + 1, figsize=(4 * (n_methods + 1), 8))\n",
        "    \n",
        "    # Original image\n",
        "    axes[0, 0].imshow(image)\n",
        "    axes[0, 0].set_title(\"Original\", fontsize=12)\n",
        "    axes[0, 0].axis('off')\n",
        "    axes[1, 0].axis('off')\n",
        "    \n",
        "    # Each method\n",
        "    for i, (method_name, heatmap) in enumerate(heatmaps.items(), 1):\n",
        "        # Heatmap\n",
        "        im = axes[0, i].imshow(heatmap, cmap='jet')\n",
        "        axes[0, i].set_title(f\"{method_name}\\nHeatmap\", fontsize=12)\n",
        "        axes[0, i].axis('off')\n",
        "        \n",
        "        # Overlay\n",
        "        overlay = apply_heatmap(image, heatmap)\n",
        "        axes[1, i].imshow(overlay)\n",
        "        axes[1, i].set_title(f\"{method_name}\\nOverlay\", fontsize=12)\n",
        "        axes[1, i].axis('off')\n",
        "    \n",
        "    if pred_info:\n",
        "        fig.suptitle(pred_info, fontsize=14, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Saved to {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def plot_prediction_bars(probs: np.ndarray, class_names: List[str] = CLASS_NAMES):\n",
        "    \"\"\"Plot prediction probabilities as a bar chart.\"\"\"\n",
        "    colors = ['green' if i == probs.argmax() else 'steelblue' for i in range(len(probs))]\n",
        "    \n",
        "    plt.figure(figsize=(8, 4))\n",
        "    bars = plt.bar(class_names, probs, color=colors, edgecolor='black')\n",
        "    plt.ylabel(\"Probability\", fontsize=12)\n",
        "    plt.xlabel(\"Class\", fontsize=12)\n",
        "    plt.title(\"Prediction Probabilities\", fontsize=14)\n",
        "    plt.ylim(0, 1)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, prob in zip(bars, probs):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
        "                 f'{prob:.3f}', ha='center', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Visualization functions defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Loading & Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to model checkpoint\n",
        "MODEL_PATH = PROJECT_ROOT / \"models\" / \"v1\" / \"efficientnet\" / \"efficientnet_deepfake_inference.pth\"\n",
        "\n",
        "# Initialize model\n",
        "model = EfficientNetDeepfake(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "# Load weights if available\n",
        "if MODEL_PATH.exists():\n",
        "    state_dict = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=True)\n",
        "    model.load_state_dict(state_dict)\n",
        "    print(f\"✓ Model loaded from {MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"✗ Model not found at {MODEL_PATH}\")\n",
        "    print(\"  Using random weights for demonstration.\")\n",
        "\n",
        "model.eval()\n",
        "print(f\"\\nModel architecture: {BACKBONE_NAME}\")\n",
        "print(f\"Number of classes: {NUM_CLASSES}\")\n",
        "print(f\"Feature dimension: {model.feat_dim}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize explainers\n",
        "# For GradCAM, we need to specify the target layer\n",
        "# For EfficientNet-B0, conv_head is the last convolutional layer\n",
        "target_layer = model.backbone.conv_head\n",
        "print(f\"GradCAM target layer: {type(target_layer).__name__}\")\n",
        "\n",
        "# Initialize explainers\n",
        "gradcam_explainer = GradCAMExplainer(model, target_layer=target_layer, device=DEVICE)\n",
        "lime_explainer = LIMEExplainer(model, device=DEVICE, num_samples=500)  # Reduce for speed\n",
        "\n",
        "print(\"\\n✓ Explainers initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Single Image Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_image(image_path: str, run_lime: bool = False):\n",
        "    \"\"\"\n",
        "    Complete analysis pipeline for a single image.\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        run_lime: Whether to run LIME (slower)\n",
        "    \"\"\"\n",
        "    print(f\"Analyzing: {image_path}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Load and preprocess\n",
        "    img = load_image(image_path)\n",
        "    input_tensor = preprocess_image(img).to(DEVICE)\n",
        "    display_img = denormalize(input_tensor)\n",
        "    \n",
        "    # Get prediction\n",
        "    pred_class, probs = gradcam_explainer.predict(input_tensor)\n",
        "    pred_label = CLASS_NAMES[pred_class]\n",
        "    confidence = probs[pred_class]\n",
        "    \n",
        "    print(f\"\\nPrediction: {pred_label}\")\n",
        "    print(f\"Confidence: {confidence:.4f}\")\n",
        "    print(f\"\\nAll probabilities:\")\n",
        "    for name, prob in zip(CLASS_NAMES, probs):\n",
        "        marker = \"←\" if name == pred_label else \"\"\n",
        "        print(f\"  {name}: {prob:.4f} {marker}\")\n",
        "    \n",
        "    # Plot prediction bars\n",
        "    plot_prediction_bars(probs)\n",
        "    \n",
        "    # Generate explanations\n",
        "    heatmaps = {}\n",
        "    \n",
        "    print(\"\\nGenerating GradCAM...\")\n",
        "    heatmaps[\"GradCAM\"] = gradcam_explainer.explain(input_tensor, target_class=pred_class)\n",
        "    \n",
        "    if run_lime:\n",
        "        print(\"Generating LIME (this may take a minute)...\")\n",
        "        heatmaps[\"LIME\"] = lime_explainer.explain(input_tensor, target_class=pred_class)\n",
        "    \n",
        "    # Plot comparison\n",
        "    pred_info = f\"Prediction: {pred_label} ({confidence:.2%})\"\n",
        "    plot_method_comparison(display_img, heatmaps, pred_info=pred_info)\n",
        "    \n",
        "    return pred_class, probs, heatmaps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage - replace with your actual image path\n",
        "# image_path = \"../data/raw/test_image.jpg\"\n",
        "# results = analyze_image(image_path, run_lime=True)\n",
        "\n",
        "print(\"Ready to analyze images!\")\n",
        "print(\"Usage: results = analyze_image('path/to/image.jpg', run_lime=True)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Batch Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batch_analyze(image_paths: List[str], output_dir: str = None, run_lime: bool = False):\n",
        "    \"\"\"\n",
        "    Analyze multiple images and collect results.\n",
        "    \n",
        "    Args:\n",
        "        image_paths: List of image file paths\n",
        "        output_dir: Directory to save visualization results\n",
        "        run_lime: Whether to run LIME analysis\n",
        "        \n",
        "    Returns:\n",
        "        List of result dictionaries\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    if output_dir:\n",
        "        output_path = Path(output_dir)\n",
        "        output_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    for i, img_path in enumerate(image_paths):\n",
        "        print(f\"\\n[{i+1}/{len(image_paths)}] Processing: {img_path}\")\n",
        "        \n",
        "        try:\n",
        "            img = load_image(img_path)\n",
        "            input_tensor = preprocess_image(img).to(DEVICE)\n",
        "            display_img = denormalize(input_tensor)\n",
        "            \n",
        "            # Prediction\n",
        "            pred_class, probs = gradcam_explainer.predict(input_tensor)\n",
        "            \n",
        "            # GradCAM\n",
        "            gradcam_heatmap = gradcam_explainer.explain(input_tensor, target_class=pred_class)\n",
        "            \n",
        "            # LIME (optional)\n",
        "            lime_heatmap = None\n",
        "            if run_lime:\n",
        "                lime_heatmap = lime_explainer.explain(input_tensor, target_class=pred_class)\n",
        "            \n",
        "            result = {\n",
        "                'image_path': img_path,\n",
        "                'prediction': CLASS_NAMES[pred_class],\n",
        "                'confidence': probs[pred_class],\n",
        "                'probabilities': probs.tolist(),\n",
        "                'gradcam_heatmap': gradcam_heatmap,\n",
        "                'lime_heatmap': lime_heatmap,\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "            # Save visualization\n",
        "            if output_dir:\n",
        "                heatmaps = {'GradCAM': gradcam_heatmap}\n",
        "                if lime_heatmap is not None:\n",
        "                    heatmaps['LIME'] = lime_heatmap\n",
        "                \n",
        "                save_path = output_path / f\"analysis_{i:03d}.png\"\n",
        "                pred_info = f\"{CLASS_NAMES[pred_class]} ({probs[pred_class]:.2%})\"\n",
        "                plot_method_comparison(display_img, heatmaps, pred_info=pred_info, \n",
        "                                       save_path=str(save_path))\n",
        "                plt.close()\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  Error: {e}\")\n",
        "            results.append({'image_path': img_path, 'error': str(e)})\n",
        "    \n",
        "    print(f\"\\n✓ Processed {len(results)} images\")\n",
        "    return results\n",
        "\n",
        "print(\"Batch analysis function defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_results(results: List[dict]):\n",
        "    \"\"\"Print summary statistics from batch analysis.\"\"\"\n",
        "    valid_results = [r for r in results if 'error' not in r]\n",
        "    \n",
        "    if not valid_results:\n",
        "        print(\"No valid results to summarize.\")\n",
        "        return\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"BATCH ANALYSIS SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total images: {len(results)}\")\n",
        "    print(f\"Successfully analyzed: {len(valid_results)}\")\n",
        "    print(f\"Errors: {len(results) - len(valid_results)}\")\n",
        "    \n",
        "    # Class distribution\n",
        "    class_counts = {name: 0 for name in CLASS_NAMES}\n",
        "    confidences = []\n",
        "    \n",
        "    for r in valid_results:\n",
        "        class_counts[r['prediction']] += 1\n",
        "        confidences.append(r['confidence'])\n",
        "    \n",
        "    print(f\"\\nPrediction Distribution:\")\n",
        "    for name, count in class_counts.items():\n",
        "        pct = count / len(valid_results) * 100\n",
        "        print(f\"  {name}: {count} ({pct:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nConfidence Statistics:\")\n",
        "    print(f\"  Mean: {np.mean(confidences):.4f}\")\n",
        "    print(f\"  Min:  {np.min(confidences):.4f}\")\n",
        "    print(f\"  Max:  {np.max(confidences):.4f}\")\n",
        "    print(f\"  Std:  {np.std(confidences):.4f}\")\n",
        "\n",
        "print(\"Summary function defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Results Export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_results_to_json(results: List[dict], output_path: str):\n",
        "    \"\"\"Export results to JSON file (excluding numpy arrays).\"\"\"\n",
        "    export_data = []\n",
        "    for r in results:\n",
        "        entry = {\n",
        "            'image_path': r.get('image_path'),\n",
        "            'prediction': r.get('prediction'),\n",
        "            'confidence': r.get('confidence'),\n",
        "            'probabilities': r.get('probabilities'),\n",
        "            'error': r.get('error'),\n",
        "        }\n",
        "        export_data.append(entry)\n",
        "    \n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(export_data, f, indent=2)\n",
        "    \n",
        "    print(f\"Results exported to {output_path}\")\n",
        "\n",
        "# Example:\n",
        "# export_results_to_json(results, '../reports/analysis_results.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Quick Reference\n",
        "\n",
        "### Single Image Analysis\n",
        "```python\n",
        "results = analyze_image('path/to/image.jpg', run_lime=True)\n",
        "```\n",
        "\n",
        "### Batch Analysis\n",
        "```python\n",
        "image_paths = ['img1.jpg', 'img2.jpg', 'img3.jpg']\n",
        "results = batch_analyze(image_paths, output_dir='../reports/figures', run_lime=False)\n",
        "summarize_results(results)\n",
        "export_results_to_json(results, '../reports/analysis_results.json')\n",
        "```\n",
        "\n",
        "### Manual Explanation\n",
        "```python\n",
        "img = load_image('path/to/image.jpg')\n",
        "input_tensor = preprocess_image(img).to(DEVICE)\n",
        "heatmap = gradcam_explainer.explain(input_tensor)\n",
        "plot_single_explanation(denormalize(input_tensor), heatmap)\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
